
% In this section we describe how \minesp works. To do so, we first introduce relevant definitions, followed by an overview of the algorithm. We subsequently discuss each component of the algorithm in detail.

% To describe how \minesp works, we first formally define time series and provide an overview of the algorithm. We subsequently discuss each component of the algorithm in detail.
To describe how \minesp works, we first provide an overview of the algorithm, then discuss each of its component in detail.

% % ------------------------------------------------
% \subsection{Definitions}
% % ------------------------------------------------
% % To both establish notation and clarify the sorts of data for which \minesp is applicable, we introduce the following definitions.
% % To facilitate a precise explanation of \minesp, we introduce the following definitions.
% \begin{Definition} \b{Sample.} A sample is a vector $\x \in \R^D$. $D$ is termed the sample's \b{dimensionality}. Each element of the sample is an integer represented using a number of bits $w$, termed the \b{bitwidth}. The bitwidth $w$ is shared by all elements.
% \end{Definition}
% \begin{Definition} \b{Time Series.} A time series $\X$ of length $T$ is a sequence of $T$ samples, $\x_1,\ldots,\x_T$. All samples $\x_t$ share the same bitwidth $w$ and dimensionality $D$. If $D = 1$, $\X$ is called \b{univariate}; otherwise it is \b{multivariate}.
% % , and the meaning of each dimension is consistent from sample to sample.
% \end{Definition}
% \begin{Definition} \b{Rows, Columns.} In a database context, we assume that each sample of a time series is one row and each dimension is one column. Because data arrives as samples and memory constraints may limit how many samples can be buffered, we assume that the data is stored in row-major order---i.e., such that each sample is stored contiguously.
% \end{Definition}

% ------------------------------------------------
\subsection{Overview}
% ------------------------------------------------

\minesp is a bit packing-based predictive coder. It consists of four components:
\begin{enumerate}
% \itemsep0em
\item \b{Forecasting.} \minesp employs a forecaster to predict each sample based on previous samples. It encodes the difference between the next sample and the predicted sample, which is typically closer to zero than the next sample itself.
\item \b{Bit packing.} \minesp then bit packs the errors as a ``payload'' and prepends a header with sufficient information to invert the bit packing.
\item \b{Run-length encoding.} If a block of errors is all zeros, \minesp waits for a block in which some error is nonzero and then writes out the number of all-zero blocks instead of the (otherwise empty) payload.
\item \b{Entropy coding.} \minesp Huffman codes the headers and payloads.
\end{enumerate}

These components are run on blocks of eight samples (motivated in Section~\ref{sec:bitpacking}), and can be modified to yield different compression-speed tradeoffs. Concretely, one can 1) skip entropy coding for greater speed and 2) choose between delta coding and our online learning method as forecasting algorithms. The latter is slightly slower but often improves compression.

We chose these steps since they allow for high speed and exploit the characteristics of time series. Forecasting leverages the high correlation of successive samples to reduce the entropy of the data. Run-length encoding allows for extreme compression in the (common) scenario that there is no change in the data---e.g., a user's smartphone may be stationary for many hours while the user is asleep. Our method of bit packing exploits temporal correlation in the variability of the data by using the same bitwidth for points that are within the same block. Huffman coding is not specific to time series but has low memory requirements and improves compression ratios.

 % in general is not especially geared towards time series, but our bit packing scheme exploits temporal correlation in the variability of the data by using the same bitwidth for points that are within the same block. Huffman coding is not specific to time series but has low memory requirements and improves compression ratios.

 % \textit{per se}, but provides a vectorizable and low-memory means of reducing the encoding size. Similarly, Huffman coding is not specific to time series but has low memory requirements and improves compression ratios.

% Before describing the four components in greater detail, we first provide an outline of the overall algorithms for compressing and decompressing blocks of data.

% We elaborate upon each of the four components in the following subsections, but first provide an outline of the overall compression and decompression functions.

\newcommand{\err}{\texttt{err}}
\newcommand{\nbits}{\texttt{nbits}}
\newcommand{\packed}{\texttt{packed}}
\newcommand{\buff}{$\texttt{buff}$}
\newcommand{\bytes}{\texttt{bytes}}
% \newcommand{\header}{\texttt{header}}
\newcommand{\payload}{\texttt{payload}}
\newcommand{\f}{\texttt{f}}
\newcommand{\fore}{\texttt{forecaster}}
\newcommand{\self}{\texttt{self}}

% ------------------------ compress overview

An overview of how \minesp compresses one block of samples is shown in Algorithm~\ref{algo:compress}. In lines \ref{line:bodyStart}-\ref{line:encPredictEnd}, \minesp predicts each sample based on the previous sample and any state stored by the forecasting algorithm. For the first sample in a block, the previous sample is the last element of the previous block, or zeros for the initial block. In lines \ref{line:eachColStart}-\ref{line:bodyEnd}, \minesp determines the number of bits required to store the largest error in each column and then bit packs the values in that column using that many bits. (Recall that each column is one variable of the time series). If all columns require 0 bits, \minesp continues reading in blocks until some error requires $>$0 bits (lines \ref{line:rleLoopStart}-\ref{line:rleLoopEnd}). At this point, it writes out a header of all 0s and then the number of all-zero blocks. Finally, it writes out the number of bits required by each column in the latest block as a header, and the bit packed data as a payload. Both header and payload are compressed with Huffman coding.

\begin{algorithm}[h]
% \caption{encodeBlock($\{\x_1, \ldots, \x_B \}, \vtheta $)}
\caption{encodeBlock($\{\x_1, \ldots, \x_B \}, \fore$)}
\label{algo:compress}
\begin{algorithmic}[1]

\State{Let \buff\sp be a temporary buffer}

\For {$i \leftarrow 1,\ldots,B$} \COMMENTT {For each sample} \label{line:bodyStart}
    % \State{$ \hat{\x}_i, \vtheta \leftarrow $predictAndTrain$(\x_{i-1}, \vtheta)  $}
    \State{$ \hat{\x}_i \leftarrow $ $\fore$.predict$(\x_{i-1})$}
    \State{$ \err_i \leftarrow \x_i - \hat{\x}_i  $}
    \State{$\fore$.train($\x_{i-1}$, $\x_i$, $\err_i$)} \label{line:encPredictEnd}
\EndFor
\For {$j \leftarrow 1,\ldots,D$} \COMMENTT {For each column} \label{line:eachColStart}
    \State{$ \nbits_j \leftarrow \max_i\{ $requiredNumBits$(\err_{ij}) \} $}
    \State{$ \packed_j \leftarrow $ bitPack$(\{\err_{1j},\ldots,\err_{Bj} \},\text{ }\nbits_j) $}  \label{line:bodyEnd}
\EndFor

\LineComment{Run-length encode if all errors are zero}
\If{$\nbits_j$ \texttt{==} $0$, $1 \le j \le D$}
    \Repeat  \COMMENT{Scan until end of run} \label{line:rleLoopStart}
        \State{Read in another block and run lines \ref{line:bodyStart}-\ref{line:bodyEnd} }
    \Until {$\exists_j [\nbits_j \neq 0 ]$} \label{line:rleLoopEnd}
    % \State{Read in another block and run lines \ref{line:bodyStart}-\ref{line:bodyEnd} until some nbits$_j$ != $0$}
    \State{Write $D$ $0$s as headers into \buff}
    \State{Write number of all-zero blocks as payload into \buff}
    \State{Output huffmanCode(\buff)}
\EndIf

\State{Write $\nbits_j$, $j = 1,\ldots,D$ as headers into \buff}
\State{Write $\packed_j$, $j = 1,\ldots,D$ as payload into \buff}
\State{Output huffmanCode(\buff)}

% \RETURN {$ odds_{event} - \max(odds_{noise}, odds_{next}) $}
\end{algorithmic}
\end{algorithm}

% ------------------------ decompress

\minesp begins decompression (Algorithm~\ref{algo:decomp}) by decoding the Huffman-coded bitstream into a header and a payload. Once decoded, these two components are easy to separate since the header is always first and of fixed size. If the header is all 0s, the payload indicates the length of a run of zero errors. In this case, \minesp runs the predictor until the corresponding number of samples have been predicted. Since the errors are zero, the forecaster's predictions are the true sample values. In the nonzero case, \minesp unpacks the payload using the number of bits specified for each column by the header.

\begin{algorithm}[h]
% \caption{decodeBlock(\bytes, $B$, $D$, $\vtheta$)}
\caption{decodeBlock(\bytes, $B$, $D$, $\fore$)}
\label{algo:decomp}
\begin{algorithmic}[1]

\State{$\nbits$, $\payload$ $\leftarrow$ huffmanDecode(\bytes, $B$, $D$) }

% \State{$\f \leftarrow \texttt{Forecaster()} $}
\If{$\nbits_j$ \texttt{==} $0$ $\forall j$} \COMMENT{Run-length encoded}
    \State{$\texttt{numblocks} \leftarrow $ readRunLength()}
    \For {$i \leftarrow 1,\ldots,(B $ $\cdot$ \texttt{numblocks})}
        % \State{$ \hat{\x}_i, \vtheta \leftarrow $ $\fore$.predict$(\x_{i-1}, \vtheta) $}
        \State{$ \x_i \leftarrow $ $\fore$.predict$(\x_{i-1})$}
        \State{Output $\x_i$}
        % \State{$\fore$.train($\x_{i-1}$, $\x_i$, 0)}
    \EndFor
    % \RETURN {}
% \EndIf
\Else \COMMENT{Not run-length encoded}
% \State{}
\For {$i \leftarrow 1,\ldots,B$}
    % \State{$ \hat{\x}_i, \vtheta \leftarrow $ $\fore$.predict$(\x_{i-1}, \vtheta)  $}
    \State{$ \hat{\x}_i \leftarrow $ $\fore$.predict$(\x_{i-1})$}
    \State{$ \err_i \leftarrow $unpackErrorVector$(i$, \nbits, \payload$) $}
    \State{$ \x_i \leftarrow \err_i + \hat{\x}_i  $}
    \State{Output $\x_i$}
    \State{$\fore$.train($\x_{i-1}$, $\x_i$, $\err_i$)}
\EndFor
\EndIf
\end{algorithmic}
\end{algorithm}

% ------------------------------------------------
\subsection{Forecasting}
% ------------------------------------------------

\minesp forecasting can use either delta coding or \justfire \text{ } (Fast Integer REgression), a novel online forecasting algorithm we introduce.

% ================================
\subsubsection{Delta Coding}
% ================================

Forecasting with delta coding consists of predicting each sample $\x_i$ to be equal to the previous sample $\x_{i-1}$, where $\x_{0} \triangleq \vec{0}$. This method is stateless given $\x_{i-1}$ and is extremely fast. It is particularly fast when combined with run-length encoding, since it yields a run of zero errors if and only if the data is constant. This means that decompression of runs requires only copying a fixed vector, with no additional forecasting or training. Moreover, when answering queries, one can sometimes avoid decompression entirely---e.g., one can compute the max of all samples in the run by computing the max of only the first value.

% ================================
\subsubsection{FIRE}
% ================================

Forecasting with \fire is slightly more expensive than delta coding but often yields better compression.
The basic idea of \fire is to model each value as a linear combination of a fixed number of previous values and learn the coefficients of this combination. Specifically, we learn an autoregressive model of the form:
% Consider a linear model for predicting a given value $x_i$ of some variable at time step $i$.
\begin{align}
    x_i = a x_{i-1} + b x_{i-2} + \eps_i
\end{align}
where $x_i$ denotes the value of some variable at time step $i$ and $\eps_i$ is a noise term.

Different values of $a$ and $b$ are suitable for different data characterisics. If $a = 2$, $b = -1$, we obtain double-delta coding, which extrapolates linearly from the previous two points and works well when the time series is smooth. If $a = 1$, $b = 0$, we recover delta coding, which models the data as a random walk. If $a = \frac{1}{2}$, $b = \frac{1}{2}$, we predict each value to be the average of the previous two values, which is optimal if the $x_i$ are i.i.d. Gaussians. In other words, these cases are appropriate for successively noisier data.

% The idea of \fire is to learn online what the best coefficients are.

The reason \fire is effective is that it learns online what the best coefficients are for each variable.
% \Fire learns appropriate coefficients online using gradient descent with L1 loss.
% To make prediction and learning as efficient as possible, however, \justfire
To make prediction and learning as efficient as possible, \justfire \text{} restricts the coefficients to lie within a useful subspace. Specifically, we exploit the observation that all of the above cases can be written as:
\begin{align}
    x_i = x_{i-1} + \alpha x_{i-1} - \alpha x_{i-2} + \eps_i
\end{align}
for $\alpha \in [-\frac{1}{2}, 1]$. Letting $\delta_i \triangleq x_i - x_{i-1}$ and subtracting $x_{i-1}$ from both sides, this is equivalent to
% \begin{align}
%     x_i = x_{i-1} + \delta_i + \alpha (x_{i-1} - x_{i-2}) + \eps_i
% \end{align}
% . Subtracting $x_{i-1}$ from both sides, we obtain
\begin{align}
    \delta_i &= \alpha \delta_{i-1} + \eps_i
\end{align}
% \begin{align}
%     \delta_i &= \delta_{i-1} + \alpha  + \eps_i \\
% \end{align}
% and therefore
% \begin{align}
%     \delta_i &= \alpha \delta_{i-1} \eps_i \\
% \end{align}
This means that we can capture all of the above cases by predicting the next delta as a rescaled version of the previous delta. This requires only a single addition and multiplication, and reduces the learning problem to that of finding a suitable value for a single parameter.

% % ------------------------------------------------
% \subsubsection{Details}
% % ------------------------------------------------
% Pseudocode for \fire prediction and training is given in Algorithm~\ref{algo:xff}.

% To make this underlying model practical and performant for integers,
% : \fire initialization (line~\ref{line:xffCtor}), prediction (line~\ref{line:xffPredict}), and training (line~\ref{line:xffTrain}).
To train and predict using this model, we use the functions shown in Algorithm~\ref{algo:xff}. First, to initialize a \fire forecaster, one must specify three values: the number of columns $D$, the learning rate $\eta$, and the bitwidth $w$ of the integers stored in the columns. Internally, the forecaster also maintains an accumulator for each column (line~\ref{line:counter}) and the difference (delta) between the two most recently seen samples (line~\ref{line:deltas}). The accumulator is a scaled version of the current $\alpha$ value with a bitwidth of $2w$. It enables fast updates of $\alpha$ with greater numerical precision than would be possible if modifying $\alpha$ directly. The accumulators and deltas are both initialized to zeros. % updates without the expense of division or multiplication instructions.

% ------------------------ xff pseudocode

\begin{algorithm}[h]
% \begin{struct}[h]
% \label{algo:xff}
% \floatname{algorithm}{Algorithm}
\caption{FIRE\_Forecaster Class} \label{algo:xff}
\begin{algorithmic}[1]

\Function{Init}{$D$, $\eta$, $w$} \label{line:xffCtor}
\State $\self$.learnShift $\leftarrow \lg(\eta)$
\State $\self$.bitWidth $\leftarrow w$ \COMMENT{8-bit or 16-bit}
\State $\self$.accumulators $\leftarrow $ zeros($D$) \label{line:counter}
\State $\self$.deltas $\leftarrow $ zeros($D$) \label{line:deltas}
\EndFunction

\Function{Predict}{$\x_{i-1}$} \label{line:xffPredict}
\State $\texttt{alphas} \leftarrow \self$.accumulators \rshift $\self$.learnShift
% \State $\texttt{alphas} \leftarrow (\texttt{alphas} \text{ }\rshift 4) \text{ }\lshift 4$
\State $\hat{\vdelta} \leftarrow$ (\texttt{alphas} $\odot$ $\self$.deltas) $\rshift \self$.bitWidth
\RETURN $\x_{i-1} + \hat{\vdelta}$
\EndFunction

\Function{Train}{$\x_{i-1}$, $\x_{i}$, $\err_i$} \label{line:xffTrain}
\State $\texttt{gradients} \leftarrow {-\sign(\err_i)} \odot \self$.deltas
% \State $\self$.accumulators $\leftarrow \self$.accumulators + $\texttt{gradients}$
% \State $\self$.accumulators \texttt{-=} $\texttt{gradients}$
\State $\self$.accumulators $\leftarrow$ $\self$.accumulators $-$ $\texttt{gradients}$
\State $\self$.deltas $\leftarrow \x_{i} - \x_{i-1}$
\EndFunction

\end{algorithmic}
\end{algorithm}


% To predict (line~\ref{line:xffPredict})
To predict, the forecaster first derives the coefficient $\alpha$ for each column based on the accumulator. By right shifting the accumulator $\log2(\eta)$ bits, the forecaster obtains a learning rate of $2^{-\log2(\eta)} = \eta$. It then estimates the next deltas as the elementwise product (denoted $\odot$) of these coefficients and the previous deltas. It predicts the next sample to be the previous sample plus these estimated deltas.

Because all values involved are integers, the multiplication is done using twice the bitwidth $w$ of the data type---e.g., using 16 bits for 8 bit data. The product is then right shifted by an amount equal to the bit width. This has the effect of performing a fixed-point multiplication with step size equal to $2^{-w}$.

% The forecaster trains (line~\ref{line:xffTrain})
The forecaster trains by performing a gradient update on the L1 loss between the true and predicted samples. I.e., given the loss:
\begin{align}
    \mathcal{L}(x_i, \hat{x}_{i}) = \abs{x_i - \hat{x}_i}
    = \abs{x_i - (x_{i-1} + \frac{\alpha}{2^w} \cdot \delta_{i-1})} \\
    = \abs{\delta_{i} - \frac{\alpha}{2^w} \cdot \delta_{i-1}}
\end{align}
for one column's value $x_i = \x_{ij}$ for some $j$ and coefficient $\alpha$, the gradient is:
\begin{align}
    % x = 5
    % \frac{\partial x}{\partial \alpha} x = 5
    % \mathcal{L}(\x_i, \hat{\x}_i)
        \frac{\partial }{\partial \alpha} \abs{\delta_{i} - \frac{\alpha}{2^w} \cdot \delta_{i-1}}
% \begin{equation*}
&= \begin{cases}
        -{2^{-w}}\vdelta_{i-1} & \x_{i} > \hat{\x}_{i} \\
        {2^{-w}}\vdelta_{i-1} & \x_{i} \le \hat{\x}_{i}
\end{cases} \\
&= -\sign(\eps) \cdot {2^{-w}}\vdelta_{i-1} \\
&\propto -\sign(\eps) \cdot \vdelta_{i-1}
% \end{equation*}
\end{align}
where we define $\eps \triangleq \x_{i} - \hat{\x}_{i}$ and ignore the $2^{-w}$ as a constant that can be absorbed into the learning rate. In all experiments reported here, we set the learning rate to $\frac{1}{2}$. This value is unlikely to be ideal for any particular dataset, but preliminary experiments showed that it consistently worked reasonably well. % and was large enough to avoid zeroing out small gradients.

In practice, \fire differs from the above pseudocode in three ways. First, instead of computing the coefficient for each sample, we compute it once at the start of each block. Second, instead of performing a gradient update after each sample, we average the gradients of all samples in each block and then perform one update. Finally, we only compute a gradient for every other sample, since this has little or no effect on the accuracy and slightly improves speed.

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=\textwidth]{paper/overview}
    \caption{Overview of \minesp using a delta coding predictor.\textit{ a)} Delta coding of each column, followed by zigzag encoding of resulting errors. The maximum number of significant (nonzero) bits is computed for each column. \textit{b)} These numbers of bits are stored in a header, and the original data is stored as a (byte-aligned) payload, with leading zeros removed. When there are few columns, each column's data is stored contiguously. When there are many columns, each row is stored contiguously, possibly with padding to ensure alignment on a byte boundary.}
    % With blocks of eight samples (only four shown for clarity), this ensures that each column's data begins on a byte boundary.
    \label{fig:overview}
    \vspace{-5mm}
\end{center}
\end{figure*}

% ------------------------------------------------
\subsection{Bit Packing} \label{sec:bitpacking}
% ------------------------------------------------

An illustration of \mine's bit packing is given in Figure~\ref{fig:overview}. The prediction errors from delta coding or \fire are zigzag encoded \cite{zigzag} and then the minimum number of bits required is computed for each column. Zigzag encoding is an invertible transform that interleaves positive and negative integers such that each integer is represented by twice its absolute value, or twice its absolute value minus one for negative integers. This makes all values nonnegative and maps integers farther from zero to larger numbers.
% ). % We zigzag encode because it is extremely efficient and facilitates computing the number of bits necesasary.

Given the zigzag encoded errors, the number of bits $w^\prime$ required in each column can be computed as the bitwidth minus the fewest leading zeros in any of that column's errors. E.g., in Figure~\ref{fig:overview}a, the first column's largest encoded value is 16, represented as \texttt{00010000}, which has three leading zeros. This means that we require $w^\prime = 8 - 3 = 5$ bits to store the values in this column. One can find this value by ORing all the values in a column together and then using a built-in function such as GCC's $\texttt{\_\_builtin\_clz}$ to compute the number of leading zeros in a single assembly instruction (c.f. \cite{fastpfor}). This optimization motivates our use of zigzag encoding to make all values nonnegative.

Once the number of bits $w^\prime$ required for each column is known, the zigzag-encoded errors can be bit packed. First, \minesp writes out a header consisting of $D$ unsigned integers, one for each column, storing the bitwidths. Each integer is stored in $\log2(w)$ bits, where $w$ is the bitwidth of the data. Since there are $w+1$ possible values of $w^\prime$ (including 0), width $w-1$ is treated as a width of $w$ by both the encoder and decoder. E.g., 8-bit data that could only be compressed to 7 bits is both stored and decoded with a bitwidth of 8.

After writing the headers, \minesp takes the appropriate number of low bits from each element and packs them into the payload. When there are few columns, all the bits for a given column are stored contiguously (i.e., column-major order). When there are many columns, the bits for each \textit{sample} are stored contiguously (i.e., row-major order). In the latter case, up to seven bits of padding are added at the end of each row so that all rows begin on a byte boundary. This means that the data for each column begins at a fixed bit offset within each row, facilitating vectorization of the decompressor. The threshold for choosing between the two formats is a sample width of $32$ bits.

The reason for this threshold is as follows. Because the block begins in row-major order and we seek to reconstruct it the same way, the row-major bit packing case is more natural. For small numbers of columns, however, the row padding can significantly reduce the compression ratio. Indeed, for univariate 8-bit data, it makes compression ratios greater than 1 impossible. This gives rise to the column-major case; using a block size of eight samples and column-major order, each column's data always falls on a byte boundary without any padding. The downside of this approach is that both encoder and decoder must transpose the block. However, for up to four 8-bit columns or two 16-bit columns, this can be done quickly using SIMD shuffling instructions.\footnote{For recent processors with AVX-512 instructions, one could double these column counts, but we refrain from assuming that these instructions will be available.} This gives rise to the cutoff of 32 bit sample width for choosing between the formats.

As a minor bit packing optimization, one can store the headers for two or more blocks contiguously, so that there is one group of headers followed by one group of payloads. This allows many headers to share one set of padding bits between the headers and payload. Grouping headers does not require buffering more than one block of raw input, but it does require buffering the appropriate number of blocks of compressed output. In addition to slightly improving the compression ratio, it also enables more headers to be unpacked with a given number of vector instructions in the decompressor. Microbenchmarks show up to $10\%$ improvement in decompression speed as the number of blocks in a group grows towards eight. However, we use groups of only two in all reported experiments to ensure that our results tend towards pessimism and are applicable under even the most extreme buffer size constraints.

% ------------------------------------------------
\subsection{Entropy Coding}
% ------------------------------------------------

We entropy code the bit packed representation of each block using Huff0, an off-the-shelf Huffman coder \cite{fse}. This encoder treats individual bytes as symbols, regardless of the bitwidth of the original data. We use Huffman coding instead of Finite-State Entropy \cite{fse} or an arithmetic coding scheme since they are slower, and we never observed a meaningful increase in compression ratio.

The benefit of adding Huffman coding to bit packing stems from bit packing's inability to optimally encode individual bytes. For a given packed bitwidth $w$, bit packing models its input as being uniformly distributed over an interval of size $2^{w}$. Appropriately setting $w$ allows it to exploit the similar variances of nearby values, but does not optimally encode individual values (unless they truly are uniformly distributed within the interval). Huffman coding is complementary in that  it fails to capture relationships between nearby bytes but optimally encodes individual bytes.

We Huffman code after bit packing, instead of before, for two reasons. First, doing so is faster. This is because the bit packed block is usually shorter than the original data, so less data is fed to the Huffman coding routines. These routines are slower than the rest of \mine, so minimizing their input size is beneficial. Second, this approach increases compression. Bit packed bytes benefit from Huffman coding, but Huffman coded bytes do not benefit from bit packing, since they seldom contain large numbers of leading zeros. This absence of leading zeros is unsurprising since Huffman codes are not byte-aligned and use ones and zeros in nearly equal measure.

% 1) Huffman coding is slower than the rest of \mine, so running it on the smaller bit packed block is (usually) shorter than the original data.

% so applying it last is faster faster than Huffman coding the original data or the errors,


% ------------------------------------------------
\subsection{Vectorization}
% ------------------------------------------------

Much of \mine's speed comes from vectorization. For headers, the fixed bitwidths for each field and fixed number of fields allows for packing and unpacking with a mix of vectorized byte shuffles, shifts, and masks. For payloads, delta (de)coding, zigzag (de)coding, and \fire all operate on each column independently, and so naturally vectorize. Because the packed data for all rows is the same length and aligned to a byte boundary (in the high-dimensional case), the decoder can compute the bit offset of each column's data one time and then use this information repeatedly to unpack each row. In the low-dimensional case, all packed data fits in a single vector register which can be shuffled/masked appropriately for each possible number of columns. This is possible since there are at most four columns in this case. On an \texttt{x86} machine, bit packing and unpacking can be accelerated with the \texttt{pext} and \texttt{pdep} instructions, respectively.
