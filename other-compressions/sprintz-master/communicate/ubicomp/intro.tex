
% Time series are ubiquitous and only growing in importance thanks to the proliferation of sensor data from autonomous vehicles, smart phones, wearables, and other connected devices. Although a huge volume of this data is stored in modern databases---many of them designed specifically for time series []---there has been relatively little work on how best to compress this data.

% Alternative 1st paragraph:

% Thanks to the exponentially increasing [] number of embedded, mobile, and wearable devices producing sensor data, there is a rapidly growing need to store time series. This has given rise to numerous time series databases to manage and query this data, both in industry [] and academia [].

% In order to store the data efficiently, all of these systems require some form of compression algorithm.

% Time series are ubiquitous and only growing in importance t

Thanks to the proliferation of smartphones, wearables, autonomous vehicles, and other connected devices, it is becoming common to collect large quantities of sensor-generated time series. Once this data is centralized in servers, many tools exist to analyze and create value from it \cite{spark, mapreduce, hive, hdfs, influxDB, druid, littleTable, openTSDB}. However, centralizing it can be challenging because of power constraints on the devices collecting the data. In particular, transmitting data wirelessly is extremely power-intensive---on a representative set of chips \cite{cc2540, cc2640}, transmitting data over Bluetooth Low Energy (BLE) costs tens of \textit{milliwatts}, while computing at full power costs only tens of \textit{microwatts}, and sitting idle costs close to 1 microwatt.

One strategy for reducing this power consumption is to extract information locally and only transmit summaries \cite{guttagAnanthaEEG, socialFMRI, respawnDB}. This can be effective in some cases, but requires both a predefined use case for which a summary is sufficient and an appropriate method of constructing this summary. Devising such a method can be a significant endeavor; e.g., Verma et al. \cite{guttagAnanthaEEG} conducted a research project to identify features in their data that would allow for accurate classification of brain activity.

A complementary and more general approach is to compress the data before transmitting it \cite{socialFMRI, lachCompress, sensorTransforms, iotSignals, iotCompressCrap}. This allows arbitrary subsequent analysis and does not require elaborate summary construction algorithms. Unfortunately, existing compression methods either 1) are only applicable for specific types of data, such as timestamps \cite{gorilla, berkeleyTreeDB, fastpfor}, audio \cite{flac, shorten, aac, vorbis} or EEG \cite{guttagAnanthaEEG, eegCS} recordings; or 2) use algorithms that are ill-suited to sensor-generated time series.

More specifically, existing methods (e.g., \cite{sax, tsCompressSmartGrid, ecgCompressLossy, apca, lz4, zstd, zlib, gzip, lemireSegmentation}) violate one or more of the following design requirements:

\begin{enumerate}
% \itemsep0mm
% \item \b{Support for fast scans}. Time series workloads are not only read-heavy \cite{respawnDB, berkeleyTreeDB, influxDB}, but often necessitate scans through the data []. This means not only that high decompression speed is essential, but that it is desirable to push down queries directly to the compressed representation.
\item \b{Small block size}. On devices with only a few kilobytes of memory, it is not possible to buffer large amounts of data before compressing it. Moreover, even with more memory, buffering can add unacceptable latency; for example, a smartwatch transmitting nine axes of 8-bit motion data at 20Hz to a smartphone would need to wait $10000 / (9 \times 1 \times 20) = 56$ seconds to fill even a 10KB buffer. This precludes using this data for gesture recognition and would add tremendous user interface latency for step counting, activity recognition, or most other purposes.
\item \b{High decompression speed}. While the device collecting the data may not need to decompress it, it is desirable to have an algorithm that could also function well in a central database. This eliminates the need to transcode the data at the server and simplifies the application. In a database, time series workloads are not only read-heavy \cite{respawnDB, berkeleyTreeDB, influxDB}, but often necessitate materializing data (or downsampled versions thereof) for visualization, clustering, computing correlations, or other operations \cite{respawnDB}. At the same time, writing is often append-only \cite{gorilla, respawnDB}. As a result, decompression speed is paramount, while compression speed need only be fast enough to keep up with the rate of data ingestion.
% \item \b{Low memory}. To reduce network usage and offload computation to clients, it is desirable for the compressor to run on clients []. Unfortunately, clients producing time series data are often low-power sensors with only a few KB of RAM [].
% \item \b{High decompression speed}. While compression speed is also desirable, time series workloads are often read-heavy [] or even append-only [], meaning that decompression will run many more times than compression. Moreover, as mentioned above, compression can often be carried out at the client.
% \item \b{Exploit related columns}. Time series often consist of several signals that will almost always be accessed together. For example, one would rarely access only X-axis acceleration without Y-axis and Z-axis, or longitude without latitude. It is desirable to make accessing related columns together inexpensive.
% \item \b{Amenable to low-bitwidth integer data}. Most real-world signals are digitized using an Analog-to-Digital Converter (ADC). This means that the data can be represented as integers of at most 32 bits \cite{digikeyADCs}, and typically 16 or fewer. For example, even lossless audio codecs store only 16 bits \cite{flac, shorten}. % Furthermore, there is empirical evidence that most time series can be quantized to 6 or fewer bits with little or no loss of information []. % (as measured by misclassification rate) []. % Consequently, we focus on compressing 8b and 16b integer time series. % Data coming from sensors will almost always be digitized by an Analog-to-Digital Converter (ADC)
\item \b{Lossless}. Given that time series are almost always noisy and often oversampled, it might not seem necessary to compress them losslessly. However, noise and oversampling 1) tend to vary across applications, and 2) can be addressed in an application-specific way as a preprocessing step. Consequently, instead of assuming that some level of downsampling or some particular smoothing will be appropriate for all data, it is better for the compression algorithm to preserve what it is given and leave preprocessing up to the application developer.
\end{enumerate}

% Existing compression algorithms almost universally violate Requirement 1. This is because modern general-purpose compressors tend to require large dictionaries of previously seen byte strings to be effective \cite{lz4, snappy, gzip, zlib}. In the time series literature, existing algorithms either violate Requirement 3 through lossy compression \cite{sax, tsCompressSmartGrid, ecgCompressLossy, apca, lemireSegmentation}

The primary contribution of this work is \mine,
a compression algorithm for time series that offers state-of-the-art compression ratios and speed while also satisfying all of the above requirements. It requires $<$1KB of memory, can use blocks of data as small as eight samples, and can decompress at up to 3GB/s in a single thread. \mine's effectiveness stems from exploiting 1) temporal correlations in each variable's value and variance, and 2) the potential for parallelization across different variables, realized through the use of vector instructions. The main limitation of \minesp is that it operates directly only on integer time series. However, as we discuss in Section ~\ref{sec:floats}, straightforward preprocessing allows it to be applied to most floating point time series as well.

A key component of \mine's operation is a novel, vectorized forecasting algorithm for integers. This algorithm can simultaneously train online and generate predictions at close to the speed of \texttt{memcpy}, while significantly improving compression ratios compared to delta coding.

A second contribution is an empirical comparison of a range of algorithms currently used to compress time series, evaluated across a wide array of public datasets. We also make available code to easily reproduce these experiments, including the plots and statistical tests in the paper. To the best of our knowledge, this constitutes the largest public benchmark for time series compression.

The remainder of this paper is structured as follows. In Section~\ref{sec:problem}, we introduce relevant definitions, background, and details regarding the problem we consider. In Section~\ref{sec:relatedWork}, we survey related work and what distinguishes \mine. In Sections~\ref{sec:method} and \ref{sec:results}, we describe \minesp and evaluate it across a number of publicly available datasets. We also discuss when \minesp is advantageous relative to other approaches.

% The contribution of this work is the introduction of \mine (Sprintz PReserves INteger Time SerieZ), a lossless compression algorithm for time series that does not assume a particular application domain (e.g., music) and is suitable for execution on low-power hardware. As we show experimentally, \mine achieves higher compression ratios than any other method across a wide range of datasets while maintaining decompression speeds over 1GB/s in a single thread.

% TODO couple more sentences about what our method leverages
% TODO list additional contributions



% ================================================================
\section{Definitions and Background} \label{sec:problem}
% ================================================================

Before elaborating upon how our method works, we introduce necessary definitions and provide relevant information regarding the problem being solved.

% ------------------------------------------------
\subsection{Definitions}
% ------------------------------------------------
% To both establish notation and clarify the sorts of data for which \minesp is applicable, we introduce the following definitions.
% To facilitate a precise explanation of \minesp, we introduce the following definitions.
\begin{Definition} \b{Sample.} A sample is a vector $\x \in \R^D$. $D$ is the sample's \b{dimensionality}. Each element of the sample is an integer represented using a number of bits $w$, the \b{bitwidth}. The bitwidth $w$ is shared by all elements.
\end{Definition}
\begin{Definition} \b{Time Series.} A time series $\X$ of length $T$ is a sequence of $\text{ }T$ samples, $\x_1,\ldots,\x_T$. All samples $\x_t$ share the same bitwidth $w$ and dimensionality $D$. If $D = 1$, $\X$ is called \b{univariate}; otherwise it is \b{multivariate}.
% , and the meaning of each dimension is consistent from sample to sample.
\end{Definition}
\begin{Definition} \b{Rows, Columns.} When represented in memory, we assume that each sample of a time series is one row and each dimension is one column. Because data arrives as samples and memory constraints may limit how many samples can be buffered, we assume that the data is stored in row-major order---i.e., such that each sample is stored contiguously.
\end{Definition}

% ------------------------------------------------
\subsection{Hardware Constraints}
% ------------------------------------------------

Many connected devices are powered by batteries or harvested energy \cite{bsnChallenges}. This results in strict power budgets and, in order to satisfy them, omission of certain functionality. In particular, many devices lack hardware support for floating point operations, SIMD (vector) instructions, and integer division. Moreover, they often have no more than a few kilobytes of memory, clocks in the tens of MHz at most, and 8-, 16-, or 32-bit processors instead of 64-bit \cite{cc2540, cc2640, quark}.

In contrast, we assume that the hardware used to decompress the data does not share these limitations. It is likely a modern x86 server with SIMD instructions, gigabytes of RAM, and a multi-GHz clock. However, because the amount of data it must store and query can be large, compression ratio and decompression speed are still important.

% ------------------------------------------------
\subsection{Data Characteristics}
% ------------------------------------------------

From a compression perspective, time series have four attributes uncommon in other data.

\begin{enumerate}
    \item \textbf{Lack of exact repeats}. In text or structured records, there are many sequences of bytes---often corresponding to words or phrases---that will exactly repeat many times. This makes dictionary-based methods a natural fit. In time series, however, the presence of noise makes exact repeats less common \cite{extract, epenthesis}.
    \item \textbf{Multiple variables}. Real-world time series often consist of multiple variables that are collected and accessed together. For example, the Inertial Measurement Unit (IMU) in modern smartphones collects three-dimensional acceleration, gyroscope, and magnetometer data, for a total of nine variables sampled at each time step. These variables are also likely to be read together, since each on its own is insufficient to characterize the phone's motion. %These variables are all sampled and fed to the processor together, resulting in every ninth value being from the same variable. This means that there is often a strong lag correlation in the data (in this case with a lag of nine). % , for a total of nine variables sampled at each time step. % These variables are all sampled and fed to the processor together, resulting in every ninth value in memory order being
    \item \textbf{Low bitwidth}. Any data collected by a sensor will be digitized into an integer by an Analog-to-Digital Converter (ADC). Nearly all ADCs have a precision of 32 bits or fewer \cite{digikeyADCs}, and typically 16 or fewer of these bits are useful. For example, even lossless audio codecs store only 16 bits per sample \cite{flac, shorten}. Even data that is not collected from a sensor can often be stored using six or fewer bits without loss of performance for many tasks \cite{epenthesis, mdlIntrinsic, sax}.
    \item \textbf{Temporal correlation}. Since the real world usually evolves slowly relative to the sampling rate, successive samples of a time series tend to have similar values. However, when multiple variables are present and samples are stored contiguously, this correlation is often present only with a lag---e.g., with nine IMU variables, every ninth value is similar. Such lag correlations violate the assumptions of most compressors, which treat adjacent bytes as the most likely to be related.
    % quantization even to a mere six bits [] rarely harms classification accuracy, and quantizing to two sometimes improves it [].
\end{enumerate}

Much of the reason \minesp outperforms existing methods is that it exploits or accounts for all of these characteristics, while existing methods do not.
