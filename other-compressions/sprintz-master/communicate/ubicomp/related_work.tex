
\minesp draws upon ideas from time series compression, time series forecasting, integer compression, general-purpose compression, and high-performance computing. From a technical perspective, \minesp is unusual or unique in its abilities to:
\begin{enumerate}
    \item Bit pack with extremely small block sizes
    \item Bit pack low-bitwidth integers effectively
    \item Efficiently exploit correlation between successive samples in \textit{multivariate} time series
    \item Naturally integrate both run-length encoding and bit packing
    \item Exploit vectorized hardware through forecaster, learning algorithm, and bit packing method co-design
\end{enumerate}
% Certain other methods do one of these, but we are not aware of an existing algorithm that does two or more.
From an application persective, \minesp is distinct in that it enables higher-ratio lossless compression with far less memory and latency than competing methods.

    % \item Quickly and losslessly compress a wide variety of time series while obtaining excellent compression ratios

% TODO also reiterate what's different in terms of what it enables here


% ------------------------------------------------
\subsection{Compression of Time Series}
% ------------------------------------------------

Most work on compressing time series has focused on lossy techniques. The most common approach is to approximate the data as a sequence of low-order polynomials \cite{swab, lemireSegmentation, tsCompressSmartGrid, iotCompressCrap, apca, paa}. An alternative, commonly seen in the data mining literature, is to discretize the time series using Symbolic Aggregate Approximation (SAX) \cite{sax} or its variations \cite{isax, isax2}. These approaches are designed to preserve enough information about the time series to support indexing or specific data mining algorithms (e.g. \cite{isax, fastShapelet, hotSax}), rather than to compress the time series \textit{per se}. As a result, they are extremely lossy; a hundred-sample time series might be compressed into one or two bytes, depending on the exact discretization parameters. % This is a result of these methods being intended to facilitate certain tasks, rather compress data \textit{per se}. % , but do construct representations that can be understood as lossy encodings of the data.

% Other lossy approaches include Adaptive Piecewise Constant Approximation \cite{apca}, and Piecewise Aggregate Approximation \cite{paa}.%, and numerous other methods  that approximate time series as sequences of low-order polynomials.

For audio time series specifically, there are a large number of lossy codecs \cite{vorbis, shorten, aac, opus}, as well as a small number of lossless \cite{flac, alac} codecs. In principle, some of these could be applied to non-audio time series. However, modern codecs make such strong assumptions about the possible numbers of channels, sampling rates, bit depths, or other characteristics that it is infeasible to use them on non-audio time series.

Many fewer algorithms exist for lossless time series compression. For floating-point time series, the only algorithm of which we are aware is that of the Gorilla database \cite{gorilla}. This method XORs each value with the previous value to obtain a diff, and then bit packs the diffs. In contrast to our approach, it assumes that time series are univariate and have 64-bit floating-point elements. % The same work also describes a method of compressing integer timestamps. This consists of first delta-delta coding and then applying a similar bit packing compression approach.
% Like other databases \cite{influxDB, berkeleyTreeDB}, they delta-delta code before applying compression %Most time series databases use some form of integer compression (c.f. next section) [], generic predictive coding \cite{akumuli}, or floating-point compression methods.

For lossless compression of integer time series (including timestamps), existing approaches include directly applying general-purpose compressors \cite{respawnDB, openTSDB, chronicleDB, kairosDB, druid}, (double) delta encoding and then applying an integer compressor \cite{influxDB, gorilla}, or predictive coding and byte-packing \cite{akumuli}. These approaches can work well, but tend to offer both less compression and less speed than \mine.

% A final noteworthy method is Blosc \cite{blosc}. While not intended solely for time series (or integers), Blosc's assumption that every $k$ bytes (or bits \cite{bitshuf}) will be correlated for some $k$ is a natural fit for multivariate time series. %grouping of correlated bytes and/or bits makes it well-suited for multivariate time series.


% ------------------------ parquet
%   -currently supports snappy, GZIP, lzo
%   -hdfs also supports these; prolly others also
%   -https://www.cloudera.com/documentation/enterprise/5-6-x/topics/impala_parquet.html#parquet_compression
% ------------------------ Akumuli
%   -DFCM predictor (for both floats and ints?); XOR with prediction, then do something to pack it
%   -"The timestamp can be a simple integer or datetime in ISO 8601 format"
%   -handles int or float values
%   -"This data-structure consists of 4KB blocks"
%   -"They require large amount of memory per data stream to maintain a sliding window of previously seen samples. The larger the context size the better compression ratio gets."
%   -"if we're dealing with 100'000 time-series we'll need about 1GB of memory only for compression contexts."
% ------------------------ RespawnDB
%   -GZIP
%   -"Enabling compression adds a factor of six slowdown in BTDS performance"
% ------------------------ OpenTSDB
%   -LZ0 or snappy for floats, varbyte (1, 2, 4, or 8 bytes) for ints
%   -http://opentsdb.net/faq.html
% ------------------------ Gorilla
%    -custom F64 for values, custom delta-delta for timestamps
% ------------------------ ChronicleDB
%   -lz4
% ------------------------ BerkeleyTreeDB
%   -custom delta-delta
% ------------------------ KairosDB
%     -off-the-shelf compressors: lzo, snappy, probably others
%     -https://github.com/kairosdb/kairosdb/search?utf8=âœ“&q=compression&type=
% ------------------------ Druid
%   -"The timestamp and metric columns are simple: behind the scenes each of these is an array of integer or floating point values compressed with LZ4."
%       -"metrics" are scalars (the time series)
%   -they also have "dimensions" which are categorical cols; assign each a numeric ID and map IDs to lists of places they occur
%   -http://druid.io/docs/latest/design/segments.html
% ------------------------ InfluxDB
%   -"Timestamp encoding is adaptive and based on the structure of the timestamps that are encoded. It uses a combination of delta encoding, scaling, and compression using simple8b run-length encoding"
%   -"Floats are encoded using an implementation of the Facebook Gorilla paper."
%   -For integers: "If all ZigZag encoded values are less than (1 << 60) - 1, they are compressed using simple8b encoding. If any values are larger than the maximum then all values are stored uncompressed in the block. If all values are identical, run-length encoding is used."
%   -"Strings are encoding using Snappy compression"
%   -and before compaction (where all the above methods get used): "When a write comes in the new points are serialized, compressed using Snappy, and written to a WAL file"
%       -and note: "This means that batching points together is required to achieve high throughput performance. (Optimal batch size seems to be 5,000-10,000 points per batch for many use cases.)"
% ------------------------ LittleTable
%   -LZO1X (LZO) compression http://www.oberhumer.com/opensource/lzo/
% ------------------------ RocksDB (not a ts database, but whatever)
%   -snappy, lz4, zstd


\subsection{Compression of Integers}

% Integer compression is not as well-studied as general-purpose compression, but has seen great progress in recent years.

The fastest methods of compressing integers are generally based on bit packing---i.e., using at most $b$ bits to represent values in $\{0, 2^b-1\}$, and storing these bits contiguously \cite{bbp, pfor, fastpfor}. Since $b$ is determined by the largest value that must be encoded, naively applying this method yields limited compression. To improve it, one can encode fixed-size blocks of data at a time, so that $b$ can be set based on the largest values in a block instead of the whole dataset \cite{kGamma, pfor, fastpfor}. A further improvement is to ignore the largest few values when setting $b$ and store their omitted bits separately \cite{pfor, fastpfor}.

\minesp bit packing differs significantly from existing methods in two ways. First, it compresses much smaller blocks of samples. This reduces its throughput as compared to, e.g., \cite{fastpfor}, but significantly improves compression ratios (c.f. Section~\ref{sec:results}). This is because large values only increase $b$ for a few samples instead of for many. Second, \minesp is designed for 8 and 16-bit integers, rather than 32 or 64-bit integers. Existing methods are often inapplicable to lower-bitwidth data (unless converted to higher-bitwidth data) thanks to strong assumptions about bitwidth and data layout.
% $b$ that is even one too large significantly alters the ratio for low-bitwidth data.

A common \cite{flac, shorten} alternative to bit packing is Golomb coding \cite{golomb}, or its special case Rice coding \cite{rice}. The idea is to assume that the values follow a geometric distribution, often with a rate constant fit to the data. %, and therefore make the encoding cost linear in the magnitude of the encoded value.

Both bit packing and Golomb coding are bit-based methods in that they do not guarantee that encoded values will be aligned on byte boundaries. When this is undesirable, one can employ byte-based methods such as 4-Wise Null Suppression \cite{kGamma}, LEB128 \cite{dwarf}, or Varint-G8IU \cite{varintG8IU}. These methods reduce the number of bytes used to store each sample by encoding in a few bits how many bytes are necessary to represent its value, and then encoding only that many bytes. Some, such as Simple8B \cite{simple8b} and SIMD-GroupSimple \cite{groupSimd}, allow fractional bytes to be stored while preserving byte alignment for groups of samples. % These methods allow for efficient universal codes---that is, codes that can represent any possible integer. Universal

% Before applying any of these coding schemes, it is common to apply some transform to the raw data to make the values closer to 0. Such transforms include delta encoding, \cite{fastpfor, bbp}, delta-delta encoding \cite{influxDB}, and linear predictive coding (LPC) \cite{flac}. LPC deterministically generates a prediction for each sample based on the previous samples, and stores the error in the prediction instead of the raw value; when the errors are small, the integers stored are closer to 0. Delta coding and delta-delta coding are special cases wherein each sample is predicted to be the previous sample, or a linear extrapolation from the previous two samples, respectively.

% ------------------------------------------------
\subsection{General-Purpose Compression}
% ------------------------------------------------
% While \minesp is only designed to compress integer time series, a
A reasonable alternative to using a time series compressor would be to apply a general-purpose compression algorithm, possibly after delta coding or other preprocessing. Thanks largely to the development of Asymmetric Numeral Systems (ANS) \cite{ans} for entropy coding, general purpose compressors have advanced greatly in recent years. In particular, Zstd \cite{zstd}, Brotli \cite{brotli}, LZ4 \cite{lz4} and others have attained speed-compression tradeoffs significantly better than traditional methods such as GZIP \cite{gzip}, LZO \cite{lzo}, etc. However, these methods have much higher memory requirements than \minesp and, empirically, often do not compress as well and/or decompress as quickly.

% Also of note is Blosc \cite{blosc}, which is especially applicable to multivariate time series as a result of its grouping correlated bits and/or bytes together during preprocessing.

% ------------------------------------------------
\subsection{Predictive Filtering}
% ------------------------------------------------

% TODO move delta, double, LPC descriptions to here. Also talk about \fire.

% Predictive coding in some form is a common element of many compression algorithms. This consists of having some forecaster predict the values of the next byte(s) to be compressed and only storing the prediction error, rather than the original value. When the forecaster is better than chance, the errors will be drawn from a lower-entropy distribution than that of the raw data. In particular, they will often be tightly concentrated around zero \cite{shorten}. The original data can be reconstructed from the errors by having the decoder run the same forecaster and add the encoded errors to its predictions.

For numeric data such as time series, there are four types of predictive coding commonly in use: predictive filtering \cite{png}, delta coding \cite{fastpfor, bbp}, double-delta coding \cite{influxDB, gorilla}, and XOR-based encoding \cite{gorilla}. In predictive filtering, each prediction is a linear combination of a fixed number of recent samples. This can be understood as an autoregressive model or the application of a Finite Impulse Response (FIR) filter. When the filter is learned from the data, this is termed ``adaptive filtering.'' Many audio compressors use some form of adaptive filtering \cite{shorten, flac, aac}.

Delta coding is a special case of predictive filtering where the prediction is always the previous value. Double-delta coding, also called delta-delta coding or delta-of-deltas coding, consists of applying delta coding twice in succession. XOR-based encoding is similar to delta coding, but replaces subtraction of the previous value with the XOR operation. This modification is often desirable for floating-point data \cite{gorilla}.

Our forecasting method can be understood as a special case of adaptive filtering. While adaptive filtering is a well-studied mathematical problem in the signal processing literature, we are unaware of a practical algorithm that attains speed within an order of magnitude of our own. I.e., our method's primary novelty is as a vectorized \textit{algorithm} for fitting and predicting multivariate time series, rather than as a mathematical \textit{model} of multivariate time series. That said, it does incorporate different modeling assumptions than other compression algorithms for time series in that it reduces the model to one parameter and omits a bias term. % It considers training speed and vectorization in a way that is orthogonal to all adaptive filtering work of which we are aware. It also differs from the adpative filtering in audio codecs in that there is only one coefficient to learn and it is learned online. %. or that accounts for the subtleties of low-precision integers.

% Indeed, the only methods

% most similar method is likely that of \cite{neuralBranchPredictor}, in which a

% Moreover, existing databases almost universally use delta, double-delta, or XOR encoding, suggesting that simple  % suggesting that these approaches are the relevant benchmarks.

% Before applying any of these coding schemes, it is common to apply some transform to the raw data to make the values closer to 0. Such transforms include delta encoding, \cite{fastpfor, bbp}, delta-delta encoding \cite{influxDB}, and linear predictive coding (LPC) \cite{flac}. LPC deterministically generates a prediction for each sample based on the previous samples, and stores the error in the prediction instead of the raw value; when the errors are small, the integers stored are closer to 0. Delta coding and delta-delta coding are special cases wherein each sample is predicted to be the previous sample, or a linear extrapolation from the previous two samples, respectively.
