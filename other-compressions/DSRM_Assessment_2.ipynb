{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Research Methods\n",
    "## Assessment 2\n",
    "### Nestor Prieto Chavana\n",
    "### 3743 Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "Human Activity Recognition (HAR) is a field with considerable amount of research in its history, however its application in a fitness context is relatively new. The positive impact that regular physical activity has in an individual's health has been documented in many research studies for decades, showing that regular exercise reduces the risk of many diseases such as diabetes and cardiovascular disease. Small and practical electronic devices that measure the user's physical activity are quickly becoming commonplace, as they allow the user to monitor their activity level and seek to achieve the recommended levels of exercise to improve or maintain their health.\n",
    "\n",
    "In this report, the PAMAP2 Physical Activity Monitoring dataset will be used from this perspective, to extract actionable insights that will allow the development of HAR software and/or hardware for fitness tracking. The PAMAP2 dataset was developed with the objective of providing a standard dataset for predictive model benchmarking that better aligns with the goals of physical aerobic activity monitoring. It contains sensor data for a range of common fitness and daily physical activities performed by a number human subjects. \n",
    "\n",
    "In the context of physical activity, the following objectives have been defined for this report:\n",
    "\n",
    "- **Objective 1:** Identify attributes in the data that will allow the recognition of activity intensity, as defined by their MET equivalent, provided in [1]. The activities included in the dataset can be classified as Light, Moderate and Vigorous activities by their energy consumption. Being able to identify the intensity of activities is useful as it allows users to know if they are getting the recommended levels of physical activity.\n",
    "\n",
    "\n",
    "- **Objective 2:** Compare the performance of different sensing devices present in the dataset, when applied to the identification of physical activities. In particular, the performance of accelerometer and gyroscope data will be compared to find which one contributes the most to activity recognition. This is useful as knowing the optimal sensor configuration will allow the design of more practical, lightweight and better performing tracking devices. \n",
    "\n",
    "\n",
    "- **Objective 3** Compare the performance of the different sensor locations provided in the dataset, namely hand, ankle and chest sensors. Again this knowledge is useful in that it allows the design of better activity tracking devices.\n",
    "\n",
    "These objectives will be tested by attempting the recognition of a specific set of common fitness and daily activities, focusing on the 12 activities defined in the protocol for the gathering of the PAMAP2 dataset[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required cell: This cell needs to be executed to import the necessary libraries\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "from IPython.display import HTML, display\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t as the\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Cleanup\n",
    "\n",
    "In this section, the activity monitoring data for the different subjects are loaded into a dataframe for cleanup and preprocessing.\n",
    "\n",
    "**N.B.**  As the loading and preprocessing steps may take a long time to run, the end results have been saved as csv files and provided along with this report, already split into dev and test sets. Code to load the preprocessed data from these files has been included in the Exploratory Data Analysis section. While the code in the next few cells can be executed to validate it's correctness, it's possible to continue reviewing this report without doing it, using these files in case it's taking too long. Only the first two code cells need to be executed, as they contain data structures that will be used throughout this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required cell: This cell needs to be executed as it contains necessary data structures\n",
    "#Activity ID map\n",
    "activity_id = {0: 'transient', 1:'lying', 2:'sitting', 3:'standing',\n",
    "              4:'walking', 5:'running', 6:'cycling', 7:'Nordic walking',\n",
    "              9:'watching TV', 10:'computer work', 11:'car driving',\n",
    "              12:'ascending stairs', 13:'descending stairs', 16:'vacuum cleaning',\n",
    "              17:'ironing', 18:'folding laundry', 19:'house cleaning',\n",
    "              20:'playing soccer', 24:'rope jumping'}\n",
    "\n",
    "#Protocol Activities: lie, sit, stand, walk, run, cycle, Nordic walk, iron, vacuum cleaning, \n",
    "#rope jump, ascend and descend stairs\n",
    "protocol_acts = [1,2,3,4,5,6,7,17,16,24,12,13]\n",
    "\n",
    "#Optional Activities: watch TV, computer work, drive car, fold laundry, house cleaning, play soccer\n",
    "optional_acts = [9,10,11,18,19,20]\n",
    "\n",
    "#MET Classification of activities\n",
    "\n",
    "#lying, sitting, standing and ironing\n",
    "light_acts = [1,2,3,17]\n",
    "#vacuum cleaning, descending stairs, walking, Nordic walking and cycling\n",
    "mod_acts = [16,13,4,7,6]\n",
    "#ascending stairs, running and rope jumping\n",
    "vig_acts = [12,5,24]\n",
    "\n",
    "#Function used to classify activities\n",
    "def map_met(act_id):\n",
    "    if act_id in light_acts:\n",
    "        return 'light'\n",
    "    if act_id in mod_acts:\n",
    "        return 'moderate'\n",
    "    if act_id in vig_acts:\n",
    "        return 'vigorous'\n",
    "    \n",
    "#Making list for updating column names in dataframe\n",
    "col_names=['timestamp', 'activity_id', 'heart_rate']\n",
    "\n",
    "IMU_locations = ['hand', 'chest', 'ankle']\n",
    "IMU_data = ['tmp', 'acc_16_01', 'acc_16_02', 'acc_16_03',\n",
    "            'acc_06_01', 'acc_06_02', 'acc_06_03',\n",
    "            'gyr_01', 'gyr_02', 'gyr_03',\n",
    "            'mag_01', 'mag_02', 'mag_03',\n",
    "            'ori_01', 'ori_02', 'ori_03', 'ori_04']\n",
    "\n",
    "col_names = col_names + [item for sublist in [[dat+'_'+loc for dat in IMU_data] for loc in IMU_locations] for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the subject files are read in sequence and appended to a single dataframe. Columns are added indicating the subject ID for each data row, and their MET classification: light, moderate or vigorous activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject101.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject102.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject103.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject104.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject105.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject106.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject107.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject108.dat',\n",
    "    '../../Datasets/archive_ics/PAMAP2_Dataset/Protocol/subject109.dat'\n",
    "]\n",
    "\n",
    "pamap2 = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    sub_data = pd.read_table(file, header=None, sep='\\s+')\n",
    "    sub_data.columns = col_names\n",
    "    sub_data['sub_id'] = int(file[-5])\n",
    "    sub_data['act_level'] = sub_data['activity_id'].apply(map_met)\n",
    "    pamap2 = pamap2.append(sub_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a fist cleanup step, the activity ID 0, which according to [2] corresponds to a transient period between activities is removed from the dataframe.\n",
    "\n",
    "The next step is keeping only the activities that correspond to each subject as documented in [3]. This is to avoid any unexpected or invalid activity data from affecting results.\n",
    "\n",
    "The data is then linearly interpolated to account for missing data in some of the rows, such as the HR column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "\n",
    "#Getting indexes of activity 0\n",
    "drop_index += list(pamap2.index[pamap2['activity_id']==0])\n",
    "\n",
    "#Keep only activities as documented on file \"PerformedActivitiesSummary.pdf\"\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==1) & (pamap2['activity_id'].isin([10,20]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==2) & (pamap2['activity_id'].isin([9,10,11,18,19,20]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==3) & (pamap2['activity_id'].isin([5,6,7,9,10,11,18,19,20,24]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==4) & (pamap2['activity_id'].isin([5,9,10,11,18,19,20,24]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==5) & (pamap2['activity_id'].isin([9,11,18,20]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==6) & (pamap2['activity_id'].isin([9,11,20]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==7) & (pamap2['activity_id'].isin([9,10,11,18,19,20,24]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==8) & (pamap2['activity_id'].isin([9,11]))])\n",
    "drop_index += list(pamap2.index[(pamap2['sub_id']==9) & (pamap2['activity_id'].isin([1,2,3,4,5,6,7,9,11,12,13,16,17]))])\n",
    "\n",
    "pamap2 = pamap2.drop(drop_index)\n",
    "    \n",
    "#Interpolate data\n",
    "pamap2 = pamap2.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to account for possible transient data that was incorrectly labelled, the first and last 10 seconds of each activity instance for each user is discarded as well. This is common practice to avoid mislabelling as seen in [2,5,6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove transients, 10 seconds from the start and end of each activity\n",
    "freq = 100\n",
    "pamap2['act_block'] = ((pamap2['activity_id'].shift(1) != pamap2['activity_id']) | (pamap2['sub_id'].shift(1) != pamap2['sub_id'])).astype(int).cumsum()\n",
    "drop_index = []\n",
    "numblocks = pamap2['act_block'].max()\n",
    "for block in range(1, numblocks+1):\n",
    "    drop_index += list(pamap2[pamap2['act_block']==block].head(10 * freq).index)\n",
    "    drop_index += list(pamap2[pamap2['act_block']==block].tail(10 * freq).index)\n",
    "    \n",
    "pamap2 = pamap2.drop(drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1730615, 57)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pamap2.shape```````\n",
    "#pamap2 = pd.DataFrame(stats.zscore(pamap2))\n",
    "\n",
    "#pamap2.to_csv('pamap2.csv', header=False, float_format='%.6f', sep=' ', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Segmentation\n",
    "\n",
    "The resulting dataset after cleanup is quite unwieldy, with almost 3 million rows, and it's difficult to perform a feasible analysis directly. It's therefore necessary to segment the dataset, which for this report will be done using a sliding window technique. The data is segmented using a sliding window of 5.12 seconds, which has been found a suitable window of time to capture movement cycles, using a 1 second displacement between consecutive windows. Features are then extracted from the raw data, which will be used for analysis and creation of a predictive model.\n",
    "\n",
    "For the accelerometer and gyroscope data, the following features are extracted for each axis of each device, and correlation is calculated for pair-wise combinations of their axes.\n",
    "\n",
    "- Mean\n",
    "- Standard Deviation\n",
    "- Correlation\n",
    "\n",
    "For the HR data, the following features are extracted. HR is normalised using the subjects resting HR, found in the file SubjectInformation.pdf included with the dataset.\n",
    "\n",
    "- Mean\n",
    "- Normalised Mean\n",
    "- Standard Deviation\n",
    "- Normalised Standard Deviation\n",
    "\n",
    "For the temperature data, the following features are extracted.\n",
    "- Mean\n",
    "- Standard Deviation\n",
    "\n",
    "Similar segmentation and feature selection processes have been found successful in previous works such as [2,5,7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-9b9fabb847ed>\", line 55, in <module>\n",
      "    features.append(stats.spearmanr(window[:,19],window[:,20])[0])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/stats/stats.py\", line 3347, in spearmanr\n",
      "    a_ranked = np.apply_along_axis(rankdata, axisout, a)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/shape_base.py\", line 380, in apply_along_axis\n",
      "    res = asanyarray(func1d(inarr_view[ind0], *args, **kwargs))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/stats/stats.py\", line 5937, in rankdata\n",
      "    obs = np.r_[True, arr[1:] != arr[:-1]]\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/index_tricks.py\", line 379, in __getitem__\n",
      "    newobj = array(item, ndmin=ndmin)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 415, in _joinrealpath\n",
      "    name, _, rest = rest.partition(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "freq = 100\n",
    "windowsize = int(5.12 * freq)\n",
    "displacement = 1*freq\n",
    "\n",
    "#columns used for analysis\n",
    "columns_used = ['sub_id', 'activity_id', 'act_level', 'heart_rate',\n",
    "                'tmp_hand','acc_16_01_hand','acc_16_02_hand','acc_16_03_hand',\n",
    "                'gyr_01_hand','gyr_02_hand','gyr_03_hand',\n",
    "                'tmp_chest','acc_16_01_chest','acc_16_02_chest','acc_16_03_chest',\n",
    "                'gyr_01_chest','gyr_02_chest','gyr_03_chest',\n",
    "                'tmp_ankle','acc_16_01_ankle','acc_16_02_ankle','acc_16_03_ankle',\n",
    "                'gyr_01_ankle','gyr_02_ankle','gyr_03_ankle']\n",
    "\n",
    "#resting HR for participating subjects, used for normalisation\n",
    "sub_rest_hr = {1:75, 2:74, 3:68, 4:58, 5:70, 6:60, 7:60, 8:66, 9:54}\n",
    "\n",
    "windows = []\n",
    "\n",
    "#Sliding window of 5.12 seconds, equaling 512 instances of data, with 1 second of overlap\n",
    "for block in range(1, numblocks+1):\n",
    "    ar = np.array(pamap2[pamap2['act_block']==block][columns_used])\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        if len(ar[start_index:start_index+windowsize,:])<512:\n",
    "            break\n",
    "        windows.append(ar[start_index:start_index+windowsize,:])\n",
    "        start_index += displacement\n",
    "\n",
    "#Next the features for each window are calculated\n",
    "window_features = []\n",
    "for window in windows:\n",
    "    features = []\n",
    "    #Adding Subject ID, Activity ID and Activity Level for ground truth\n",
    "    features.append(window[0][0])\n",
    "    features.append(window[0][1])\n",
    "    features.append(window[0][2])\n",
    "    #Computing mean, normalised mean and std for HR data\n",
    "    features.append(np.mean(window[:,3]))\n",
    "    features.append(np.mean(window[:,3]/sub_rest_hr[window[0][0]]))\n",
    "    features.append(np.std(window[:,3]))\n",
    "    features.append(np.std(window[:,3]/sub_rest_hr[window[0][0]]))\n",
    "    #Computing mean and std for the rest of the columns\n",
    "    for col in range(4,25):\n",
    "        features.append(np.mean(window[:,col]))\n",
    "        features.append(np.std(window[:,col]))\n",
    "    #Computing correlation between axes for hand Accelerometer    \n",
    "    features.append(stats.spearmanr(window[:,5],window[:,6])[0])\n",
    "    features.append(stats.spearmanr(window[:,6],window[:,7])[0])\n",
    "    features.append(stats.spearmanr(window[:,5],window[:,7])[0])\n",
    "    #Computing correlation between axes for chest Accelerometer  \n",
    "    features.append(stats.spearmanr(window[:,12],window[:,13])[0])\n",
    "    features.append(stats.spearmanr(window[:,13],window[:,14])[0])\n",
    "    features.append(stats.spearmanr(window[:,12],window[:,14])[0])\n",
    "    #Computing correlation between axes for ankle Accelerometer\n",
    "    features.append(stats.spearmanr(window[:,19],window[:,20])[0])\n",
    "    features.append(stats.spearmanr(window[:,20],window[:,21])[0])\n",
    "    features.append(stats.spearmanr(window[:,19],window[:,21])[0])\n",
    "    #Computing correlation between axes for hand Gyroscope    \n",
    "    features.append(stats.spearmanr(window[:,8],window[:,9])[0])\n",
    "    features.append(stats.spearmanr(window[:,9],window[:,10])[0])\n",
    "    features.append(stats.spearmanr(window[:,8],window[:,10])[0])\n",
    "    #Computing correlation between axes for chest Gyroscope  \n",
    "    features.append(stats.spearmanr(window[:,15],window[:,16])[0])\n",
    "    features.append(stats.spearmanr(window[:,16],window[:,17])[0])\n",
    "    features.append(stats.spearmanr(window[:,15],window[:,17])[0])\n",
    "    #Computing correlation between axes for ankle Gyroscope\n",
    "    features.append(stats.spearmanr(window[:,22],window[:,23])[0])\n",
    "    features.append(stats.spearmanr(window[:,23],window[:,24])[0])\n",
    "    features.append(stats.spearmanr(window[:,22],window[:,24])[0])\n",
    "    window_features.append(features)\n",
    "\n",
    "#new dataframe with features for each window\n",
    "features = pd.DataFrame(window_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names are added to the features dataframe. Finally, the dataframe is divided into development and testing datasets with a 70-30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating column list for features dataset\n",
    "feature_names = ['sub_id','activity_id','act_level','hr_mean','hr_mean_normal','hr_std', 'hr_std_normal']\n",
    "locs = ['hand','chest','ankle']\n",
    "cols = ['tmp','acc_x','acc_y','acc_z', 'gyr_x','gyr_y','gyr_z']\n",
    "feats = ['mean','std']\n",
    "\n",
    "for loc in locs:\n",
    "    for col in cols:\n",
    "        for feat in feats:\n",
    "            feature_names.append(loc + '_' + col + '_' + feat)\n",
    "for loc in locs:\n",
    "    feature_names += [loc + '_acc_xy_cor', loc + '_acc_yz_cor', loc + '_acc_xz_cor']\n",
    "\n",
    "for loc in locs:\n",
    "    feature_names += [loc + '_gyr_xy_cor', loc + '_gyr_yz_cor', loc + '_gyr_xz_cor']\n",
    "\n",
    "features.columns = feature_names\n",
    "\n",
    "#Using a seed to facilitate replication of results\n",
    "dev_data_df = features.sample(frac=0.7, random_state=1)\n",
    "test_data_df = features.drop(dev_data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the long execution time of the pre-processing and feature extraction steps, the development and test datasets have been stored in csv files, which have been provided along with this report. This allows execution of the analysis steps without the need to perform preprocessing each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following lines to save the features dataframes to local files\n",
    "#dev_data_df.to_csv(\"dev_data\", index=False)\n",
    "#test_data_df.to_csv(\"test_data\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis\n",
    "\n",
    "With the features extracted from the raw data and partitioned into development and testing, exploratory data analysis can now be performed on our development dataset.\n",
    "\n",
    "First, an overview of the development dataset distribution will be given. The dataset is composed of 11773 windows which are each labelled with their corresponding activity id and intensity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following lines to read features from csv files\n",
    "#dev_data_df = pd.read_csv(\"dev_data\", header=0)\n",
    "#test_data_df = pd.read_csv(\"test_data\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dev_data_df.groupby('activity_id').count()['act_level']\n",
    "s = s.rename(\"Activity Counts\")\n",
    "s.index = [activity_id[x] for x in protocol_acts]\n",
    "print(('Dev Dataset by Activity'))\n",
    "display(s.sort_values(ascending =False))\n",
    "ax = s.sort_values(ascending =False).plot(kind='bar', figsize=(8,4))\n",
    "_ = ax.set_ylabel('windows')\n",
    "_ = ax.set_xlabel('activity')\n",
    "_ = ax.set_title('Test Dataset by Activity') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined by their MET will be looked for. According to [1,2], the activities in the protocol can be classified as:\n",
    "- Light Effort         (<3.0 METs): lying, sitting, standing and ironing\n",
    "- Moderate Effort (3.0 - 6.0 METs): vacuum cleaning, descending stairs, walking, Nordic walking and cycling\n",
    "- Vigorous Effort      (> 6.0 MET): ascending stairs, running and rope jumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dev_data_df.groupby('act_level').count()['activity_id']\n",
    "s = s.rename(\"Activity Level Counts\")\n",
    "print(('Dev Dataset by Activity Intensity Level'))\n",
    "display(s)\n",
    "ax = s.plot(kind='bar', figsize=(8,4))\n",
    "_ = ax.set_ylabel('windows')\n",
    "_ = ax.set_xlabel('activity level')\n",
    "_ = ax.set_title('Test Dataset by Activity Intensity Level') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions have been created to facilitate the plotting of dataframes with a specific formatting. They will be used to explore some features during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(df, title, xlabel, ylabel, xticks, yticks, showcbar=True, showannot=False):\n",
    "    \"\"\"Function used to plot heatmap using input dataframe\"\"\"\n",
    "    mycmap = LinearSegmentedColormap.from_list('mycmap', ['lightgreen', 'tomato'])\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.gca()\n",
    "    _ = sns.heatmap(df, cmap = mycmap, yticklabels=yticks, xticklabels=xticks, square=True,\\\n",
    "                    linewidths=0.01, cbar=showcbar, annot=showannot,fmt='.1f')\n",
    "    _ = ax.set_ylabel(ylabel)\n",
    "    _ = ax.set_xlabel(xlabel)\n",
    "    _ = ax.set_title(title)\n",
    "    \n",
    "def plot_series(df, feat, location, act, unit, pylim):\n",
    "    plottitle = location + ' ' + feat + ' - ' + act\n",
    "    plotx = location.lower() + '_' + feat.lower()[0:3] + '_x_mean'\n",
    "    ploty = location.lower() + '_' + feat.lower()[0:3] + '_y_mean'\n",
    "    plotz = location.lower() + '_' + feat.lower()[0:3] + '_z_mean'\n",
    "    ax1 = df.plot(x=df.index,y=plotx, color='r', figsize=(12,5), ylim=pylim)\n",
    "    _ = df.plot(x=df.index,y=ploty, color='g', ax=ax1)\n",
    "    _ = df.plot(x=df.index,y=plotz, color='b', ax=ax1)\n",
    "    _ = ax1.set_title(plottitle)\n",
    "    _ = ax1.set_xlabel('window')\n",
    "    _ = ax1.set_ylabel(unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One promising feature that could be used to classify activities by their intensity level is heart rate. In the following cell, a heat map of average heart rate by activity and subject is plotted, to explore their interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yticks = [y for x, y in activity_id.items() if x in protocol_acts]\n",
    "xticks = [1,2,3,4,5,6,7,8,9]\n",
    "ylabel = 'Activity'\n",
    "xlabel = 'Subject ID'\n",
    "\n",
    "title = 'Mean Heart Rate by Activity and Subject'\n",
    "df = dev_data_df.groupby(['activity_id','sub_id'], as_index=False).mean() \\\n",
    "            [['activity_id','sub_id','hr_mean']].pivot(index='activity_id', columns='sub_id', values='hr_mean')\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks)\n",
    "\n",
    "title = 'Normalised Mean Heart Rate by Activity and Subject'\n",
    "df = dev_data_df.groupby(['activity_id','sub_id'], as_index=False).mean() \\\n",
    "            [['activity_id','sub_id','hr_mean_normal']].pivot(index='activity_id', columns='sub_id', values='hr_mean_normal')\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same plot, now grouped by activity level and subject id is shown below. From these plots, it looks like heart rate may indeed be enough to predict activity level on its own, since it's value closely follows the division between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dev_data_df.groupby(['act_level','sub_id'], as_index=False).mean() \\\n",
    "            [['act_level','sub_id','hr_mean']].pivot(index='act_level', columns='sub_id', values='hr_mean')\n",
    "    \n",
    "yticks = ['light','moderate','vigorous']\n",
    "xticks = [1,2,3,4,5,6,7,8,9]\n",
    "ylabel = 'Intensity Level'\n",
    "xlabel = 'Subject ID'\n",
    "title = 'Mean Heart Rate by Intensity Level and Subject'\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks, showcbar=False, showannot=True)\n",
    "\n",
    "df = dev_data_df.groupby(['act_level','sub_id'], as_index=False).mean() \\\n",
    "            [['act_level','sub_id','hr_mean_normal']].pivot(index='act_level', columns='sub_id', values='hr_mean_normal')\n",
    "title = 'Normalised Mean Heart Rate by Intensity Level and Subject'\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks, showcbar=False, showannot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature that might prove useful in activity classification is temperature. Below, the mean temperature readings for the different sensor locations are plotted by activity and subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yticks = [y for x, y in activity_id.items() if x in protocol_acts]\n",
    "xticks = [1,2,3,4,5,6,7,8,9]\n",
    "ylabel = 'Activity'\n",
    "xlabel = 'Subject ID'\n",
    "\n",
    "title = 'Hand Temp by Activity and Subject'\n",
    "df = dev_data_df.groupby(['activity_id','sub_id'], as_index=False).mean() \\\n",
    "            [['activity_id','sub_id','hand_tmp_mean']].pivot(index='activity_id', columns='sub_id', values='hand_tmp_mean')\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks)\n",
    "\n",
    "title = 'Chest Temp by Activity and Subject'\n",
    "df = dev_data_df.groupby(['activity_id','sub_id'], as_index=False).mean() \\\n",
    "            [['activity_id','sub_id','chest_tmp_mean']].pivot(index='activity_id', columns='sub_id', values='chest_tmp_mean')\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks)\n",
    "\n",
    "title = 'Ankle Temp by Activity and Subject'\n",
    "df = dev_data_df.groupby(['activity_id','sub_id'], as_index=False).mean() \\\n",
    "            [['activity_id','sub_id','ankle_tmp_mean']].pivot(index='activity_id', columns='sub_id', values='ankle_tmp_mean')\n",
    "plot_heatmap(df, title, xlabel, ylabel, xticks, yticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the average temperature plots, it looks like it might not be that useful for differentiating between activities. Indeed, it's value seems more dependent on the subject than the activity itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the available features are composed of sensor readings for different types of devices, specifically accelerometer and gyroscopes. One of each sensor was located at hand, chest and ankle locations. The readings from these devices should provide suitable patterns for each activity so that a classifier model is able to identify them using these features.\n",
    "\n",
    "In the next cell, accelerometer readings for each location are visualized for three different activities of increasing intensity: lying, walking and running. The objective is that the patterns created by each activity should be different enough to make that activity recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1=dev_data_df[dev_data_df['activity_id']==1]\n",
    "df2=dev_data_df[dev_data_df['activity_id']==4]\n",
    "df3=dev_data_df[dev_data_df['activity_id']==5]\n",
    "\n",
    "feat = 'Accelerometer'\n",
    "unit = 'ms^-2'\n",
    "pylimit = (-25,25)\n",
    "\n",
    "plot_series(df1, feat, 'Hand', 'Lying', unit, pylimit)\n",
    "plot_series(df2, feat, 'Hand', 'Walking', unit, pylimit)\n",
    "plot_series(df3, feat, 'Hand', 'Running', unit, pylimit)\n",
    "\n",
    "plot_series(df1, feat, 'Chest', 'Lying', unit, pylimit)\n",
    "plot_series(df2, feat, 'Chest', 'Walking', unit, pylimit)\n",
    "plot_series(df3, feat, 'Chest', 'Running', unit, pylimit)\n",
    "\n",
    "plot_series(df1, feat, 'Ankle', 'Lying', unit, pylimit)\n",
    "plot_series(df2, feat, 'Ankle', 'Walking', unit, pylimit)\n",
    "plot_series(df3, feat, 'Ankle', 'Running', unit, pylimit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signals captured from each accelerometer location appear to be distinguishable enough, though it might be necessary to consider more than one location at a time in order to classify them correctly.\n",
    "\n",
    "Next, the exercise is repeated with the readings from the gyroscope sensors, for the same locations and activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1=dev_data_df[dev_data_df['activity_id']==1]\n",
    "df2=dev_data_df[dev_data_df['activity_id']==4]\n",
    "df3=dev_data_df[dev_data_df['activity_id']==5]\n",
    "\n",
    "feat = 'Gyroscope'\n",
    "unit = 'rad/s'\n",
    "pylimit = (-1.5,1.5)\n",
    "\n",
    "plot_series(df1, feat, 'Hand', 'Lying', unit, pylimit)\n",
    "plot_series(df2, feat, 'Hand', 'Walking', unit, pylimit)\n",
    "plot_series(df3, feat, 'Hand', 'Running', unit, pylimit)\n",
    "\n",
    "plot_series(df1, feat, 'Chest', 'Lying', unit, pylimit)\n",
    "plot_series(df2, feat, 'Chest', 'Walking', unit, pylimit)\n",
    "plot_series(df3, feat, 'Chest', 'Running', unit, pylimit)\n",
    "\n",
    "plot_series(df1, feat, 'Ankle', 'Lying', unit, pylimit)\n",
    "plot_series(df2, feat, 'Ankle', 'Walking', unit, pylimit)\n",
    "plot_series(df3, feat, 'Ankle', 'Running', unit, pylimit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gyroscope readings appear harder to differentiate to the naked eye, but they might still be useful for training classifier models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hypothesis Development and Testing\n",
    "\n",
    "The following class has been created to facilitate the calculations necessary for testing the hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class meandiffanalysis:\n",
    "    \"\"\"\n",
    "    This class is used to perform a statistical analysis of the difference of means between two samples. \n",
    "    It receives a dictionary with the statistics of each of the samples, namely  mean, median, std, total items and \n",
    "    name.\n",
    "    \n",
    "    The class has several methods that use these statistics to calculate the difference between the sample means,\n",
    "    as well as the p-values using either normal or t distributions.\n",
    "    \n",
    "    Each method displays its result as HTML to create more visually attractive output.\n",
    "    \"\"\"   \n",
    "    \n",
    "    def __init__(self, datadict):\n",
    "        \"\"\"Assign dicitionary values to local variables\"\"\"\n",
    "        self.sam1 = datadict['sam1']\n",
    "        self.sam2 = datadict['sam2']\n",
    "        self.mean1 = datadict['mean1']\n",
    "        self.median1 = datadict['median1']\n",
    "        self.std1 = datadict['std1']\n",
    "        self.count1 = datadict['count1']\n",
    "        self.mean2 = datadict['mean2']\n",
    "        self.median2 = datadict['median2']\n",
    "        self.std2 = datadict['std2']\n",
    "        self.count2 = datadict['count2']\n",
    "        self.nullhyp = datadict['nullhyp']\n",
    "    \n",
    "    def displaystattable(self):\n",
    "        \"\"\"Displays a table with the basic statistics of the two samples being compared\"\"\"\n",
    "        display(HTML(\"<table><tr><th></th><th>Mean</th><th>Median</th><th>Std Dev</th><th>Count</th></tr>\"\n",
    "                 \"<tr><th>{}</th><td>{:,.4f}</td><td>{:,.4f}</td><td>{:,.4f}</td><td>{:,}</td></tr>\"\n",
    "                 \"<tr><th>{}</th><td>{:,.4f}</td><td>{:,.4f}</td><td>{:,.4f}</td><td>{:,}</td></tr></table><br>\"\n",
    "                 .format(self.sam1, self.mean1, self.median1, self.std1, self.count1, self.sam2, self.mean2, self.median2, self.std2, self.count2)))\n",
    "    \n",
    "    def displayhypotheses(self):\n",
    "        \"\"\"Displays the null and alternative hypotheses\"\"\"\n",
    "        display(HTML(\"<b>Null Hypothesis</b><br>H<sub>0</sub>: μ<sub>1</sub> -  μ<sub>2</sub> = {}\".format(self.nullhyp)))\n",
    "        display(HTML(\"<b>Alternative Hypothesis</b><br>H<sub>1</sub>: μ<sub>1</sub> -  μ<sub>2</sub> > {}\".format(self.nullhyp)))\n",
    "    \n",
    "    def displaymeandiff_normal(self):\n",
    "        \"\"\"Calculates and displays the difference between the mans of two samples\"\"\"\n",
    "        diff = self.mean1-self.mean2\n",
    "        comberr = np.sqrt(self.std1**2/self.count1 + self.std2**2/self.count2)\n",
    "        self.zval = diff/comberr\n",
    "        display(HTML(\"<b>Difference between means</b><br>\"\n",
    "                \"D ~ N (μ<sub>1</sub> - μ<sub>2</sub> , \"\n",
    "                 \"&#x03C3<sub>1</sub><sup>2</sup> / n<sub>1</sub> + \"\n",
    "                 \"&#x03C3<sub>2</sub><sup>2</sup> / n<sub>2</sub>)<br>\"\n",
    "                 \"P(D ≥ {}) = P( Z ≥ {} / √( \"\n",
    "                 \"&#x03C3<sub>1</sub><sup>2</sup> / n<sub>1</sub> + \"\n",
    "                 \"&#x03C3<sub>2</sub><sup>2</sup> / n<sub>2</sub>) )<br>\"\n",
    "                 \"P(D ≥ {}) = P( Z ≥ {})\".format(diff,diff,diff,self.zval)))\n",
    "    \n",
    "    def displaypvalue_normal(self):\n",
    "        \"\"\"Calculates and displays p-value using the normal distribution\"\"\"\n",
    "        pval = 1 - norm.cdf(self.zval)\n",
    "        display(HTML(\"<b>P-value of difference using normal distribution</b><br>{}\".format(pval)))\n",
    "    \n",
    "    def displaymeandiff_t(self):\n",
    "        \"\"\"Calculates and displays the difference between the mans of two samples using t-test\"\"\"\n",
    "        diff = self.mean1-self.mean2\n",
    "        df = self.count1 + self.count2 - 2\n",
    "        pools = np.sqrt((((self.count1-1)*self.std1**2) + ((self.count2-1)*self.std2**2))/df)\n",
    "        se = pools * np.sqrt(1/self.count1 + 1/self.count2)\n",
    "        self.tval = diff / se         \n",
    "        display(HTML(\"<b>Two-sample T-test</b><br>\"\n",
    "                \"DF = (n<sub>1</sub> + n<sub>2</sub> - 2 ) <br>\"\n",
    "                \"DF = {}<br><br>\"\n",
    "                \"s = √ ((n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> / DF )<br>\"\n",
    "                \"s = {}<br><br>\"\n",
    "                \"SE = s * √ ( 1 / n<sub>1</sub> + 1 / n<sub>2</sub> )<br>\"\n",
    "                \"SE = {}<br><br>\"\n",
    "                \"Diff = μ<sub>1</sub> - μ<sub>2</sub><br>\"\n",
    "                \"Diff = {}<br><br>\"\n",
    "                \"t-statistic = Diff / SE<br>\"\n",
    "                \"t-statistic = {}<br>\".format(df, pools, se, diff, self.tval)))\n",
    "        \n",
    "    def displaypvalue_t(self):\n",
    "        \"\"\"Calculates and displays p-value using the t distribution.\"\"\"\n",
    "        df = self.count1 + self.count2 - 2\n",
    "        pval = 1 - the.cdf(self.tval, df)\n",
    "        display(HTML(\"<b>P-value of difference using T distribution</b><br>{}\".format(pval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of hypotheses to be tested concerns the first objective stated for this report. During EDA, the HR data was identified as having the potential to allow classification of activities according to their MET equivalent. To make sure this assessment is correct, a set of two hypotheses regarding the mean of HR data for each MET classification will be tested, specifically, that the mean of their HR data should follow the pattern Light<Moderate<Vigorous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis 1a: If HR is correlated with Activity Intensity, then Moderate activities should have a higher mean HR than Light activities.\n",
    "\n",
    "The first hypothesis to be tested states that the HR mean of Moderate effort activities should be higher than that of light activities. Suitable data for testing is first extracted from the test dataset, and a meandiffanalysis object is created to perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data_df\n",
    "df['act_level'] = df['activity_id'].apply(map_met)\n",
    "\n",
    "#The following two samples of genre will be compared\n",
    "sam1 = 'moderate'\n",
    "sam2 = 'light'\n",
    "\n",
    "#Create dictionary with the necessary statistics to perform the calculation\n",
    "datadict = {}\n",
    "datadict['sam1'] = sam1\n",
    "datadict['sam2'] = sam2\n",
    "datadict['mean1'] = df[df.act_level==sam1].hr_mean_normal.mean()\n",
    "datadict['median1'] = df[df.act_level==sam1].hr_mean_normal.median()\n",
    "datadict['std1'] = df[df.act_level==sam1].hr_mean_normal.std()\n",
    "datadict['count1'] = df[df.act_level==sam1].hr_mean_normal.count()\n",
    "datadict['mean2'] = df[df.act_level==sam2].hr_mean_normal.mean()\n",
    "datadict['median2'] = df[df.act_level==sam2].hr_mean_normal.median()\n",
    "datadict['std2'] = df[df.act_level==sam2].hr_mean_normal.std()\n",
    "datadict['count2'] = df[df.act_level==sam2].hr_mean_normal.count()\n",
    "datadict['nullhyp'] = 0\n",
    "\n",
    "#Display statistics\n",
    "h1a=meandiffanalysis(datadict)\n",
    "h1a.displaystattable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both samples are of a sufficient size to assume a normal distribution, hence this will be used to calculate the p-value. The null hypothesis will be that the difference between means should equal 0. The alternative hypothesis will be a difference higher than 0. The significance level used will be 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display analysis results\n",
    "h1a.displayhypotheses()\n",
    "h1a.displaymeandiff_normal()\n",
    "h1a.displaypvalue_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a resulting p-value of 0, the null hypothesis can be rejected. As such it can be concluded with good confidence that Moderate effort activities will have a higher mean HR than light effort activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis 1b: If HR is correlated with Activity Intensity, then Vigorous activities should have a higher mean HR than Moderate activities.\n",
    "\n",
    "Complementing the previous exercise, the next hypothesis to be tested will be whether the HR mean of Vigorous effort activities is in general higher than that of Moderate activities. Again suitable data for testing is extracted from the test dataset, and a meandiffanalysis object is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data_df\n",
    "df['act_level'] = df['activity_id'].apply(map_met)\n",
    "\n",
    "#The following two samples of genre will be compared\n",
    "sam1 = 'vigorous'\n",
    "sam2 = 'moderate'\n",
    "\n",
    "#Create dictionary with the necessary statistics to perform the calculation\n",
    "datadict = {}\n",
    "datadict['sam1'] = sam1\n",
    "datadict['sam2'] = sam2\n",
    "datadict['mean1'] = df[df.act_level==sam1].hr_mean_normal.mean()\n",
    "datadict['median1'] = df[df.act_level==sam1].hr_mean_normal.median()\n",
    "datadict['std1'] = df[df.act_level==sam1].hr_mean_normal.std()\n",
    "datadict['count1'] = df[df.act_level==sam1].hr_mean_normal.count()\n",
    "datadict['mean2'] = df[df.act_level==sam2].hr_mean_normal.mean()\n",
    "datadict['median2'] = df[df.act_level==sam2].hr_mean_normal.median()\n",
    "datadict['std2'] = df[df.act_level==sam2].hr_mean_normal.std()\n",
    "datadict['count2'] = df[df.act_level==sam2].hr_mean_normal.count()\n",
    "datadict['nullhyp'] = 0\n",
    "\n",
    "#Display statistics\n",
    "h1b=meandiffanalysis(datadict)\n",
    "h1b.displaystattable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the normal distribution will be employed to calculate p-values. The null hypothesis will be that the difference between means in samples should equal 0. The alternative hypothesis will be a difference higher than 0. The significance level used will be 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Display analysis results\n",
    "h1b.displayhypotheses()\n",
    "h1b.displaymeandiff_normal()\n",
    "h1b.displaypvalue_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a resulting p-value of 0, the null hypothesis can be rejected. It can be concluded with good confidence that Vigorous effort activities will have a higher mean HR than Moderate effort activities.\n",
    "\n",
    "From the result of these tests it can be reasonably concluded that HR data might be enough to identify activities in the dataset by their MET equivalent. This would be convenient since it would mean that the sensor setup necessary for a tracking  device would be simplified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Development and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, a set of prediction models will be developed and tested in order to answer the questions stated in the objectives of this report. Firs, an overview of the test dataset is given in the following cells\n",
    "\n",
    "The test dataset is composed of 5046 activity windows, each labelled with one activity id and it's corresponding intensity level. The distributions of the different activities and activity levels present in the dataset are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = test_data_df.groupby('activity_id').count()['act_level']\n",
    "s = s.rename(\"Activity Counts\")\n",
    "s.index = [activity_id[x] for x in protocol_acts]\n",
    "print(('Test Dataset by Activity'))\n",
    "display(s.sort_values(ascending =False))\n",
    "ax = s.sort_values(ascending =False).plot(kind='bar', figsize=(8,4))\n",
    "_ = ax.set_ylabel('windows')\n",
    "_ = ax.set_xlabel('activity')\n",
    "_ = ax.set_title('Test Dataset by Activity') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = test_data_df.groupby('act_level').count()['activity_id']\n",
    "s = s.rename(\"Activity Level Counts\")\n",
    "print(('Test Dataset by Activity Intensity Level'))\n",
    "display(s)\n",
    "ax = s.plot(kind='bar', figsize=(8,4))\n",
    "_ = ax.set_ylabel('windows')\n",
    "_ = ax.set_xlabel('activity level')\n",
    "_ = ax.set_title('Test Dataset by Activity Intensity Level') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function has been created to display a confusion matrix, and calculate it's associated statistics of accuracy, precision, recall and f-score. It will be used to compare the performance results of different classification models and feature selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predict_labels, real_labels, cats, title):\n",
    "    \"\"\"\n",
    "    Function used to display confusion matrix from a set of predicted and real labels. Displays a confusion matrix,\n",
    "    then uses sklearn library utilities to calculate accuracy, precision, recall and F-score from the data.\n",
    "    \n",
    "    Params:\n",
    "    predict_labels: array of labels predicted by model\n",
    "    real_labels: array of real labels\n",
    "    cats: categories for rows and columns of confusion matrix. If false, activity data from protocol activities are used.\n",
    "    title: title of confusion matrix.\n",
    "    \n",
    "    \"\"\" \n",
    "    pred_results = {}\n",
    "    base_dict = {}\n",
    "    \n",
    "    #If cats parameter is False, the matrix is created with activity data from protocol activities\n",
    "    if not cats:\n",
    "        for i in protocol_acts:\n",
    "            base_dict[i]=0\n",
    "        for i in protocol_acts:\n",
    "            pred_results[i] = base_dict.copy()\n",
    "    else:\n",
    "        for i in cats:\n",
    "            base_dict[i]=0\n",
    "        for i in cats:\n",
    "            pred_results[i] = base_dict.copy()\n",
    "    \n",
    "    #Dictionary is created counting real values for predicted labels\n",
    "    for pl,tl in list(zip(predict_labels,  real_labels)):\n",
    "        pred_results[pl][tl]+=1\n",
    "\n",
    "    pred_results_df = pd.DataFrame(pred_results)\n",
    "    if not cats:\n",
    "        pred_results_df.columns=[activity_id[x] for x in protocol_acts]\n",
    "        pred_results_df.index = [activity_id[x] for x in protocol_acts]\n",
    "    \n",
    "    #Accuracy, precision, recall and f-score are calculated using sklearn library\n",
    "    precision = precision_score(real_labels, predict_labels, average='macro')\n",
    "    recall = recall_score(real_labels, predict_labels, average='macro')\n",
    "    accuracy = accuracy_score(real_labels, predict_labels)\n",
    "    fscore = f1_score(real_labels, predict_labels, average='macro')\n",
    "    \n",
    "    #Display results\n",
    "    print((title))\n",
    "    display((pred_results_df))\n",
    "    print(('Accuracy: ' + str(accuracy)))\n",
    "    print(('Precision: ' + str(precision)))\n",
    "    print(('Recall: ' + str(recall)))\n",
    "    print(('F-score: ' + str(fscore)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective 1\n",
    "\n",
    "The first objective set out at the start of this report was to identify attributes in the data that will allow the recognition of activity intensity, as defined by their MET equivalent. During the EDA portion of the report, the HR data was identified as a possible candidate, since its mean value appeared to closely follow the intensity of an activity. \n",
    "\n",
    "This hypothesis was then formally tested in the previous section, with the results seeming to confirm this assessment. Results seemed to indicate that there was enough difference between the HR mean of activities classified by their MET equivalent, that this attribute could be used to differentiate between these classes of activities.\n",
    "\n",
    "However, there is still matter of being able to make this prediction with an acceptable level of accuracy. Is HR data enough to classify activity intensity with high accuracy? This will be put to the test in the following section. For this purpose, the scikit-learn tree classifier will be used, as this type of classifier has shown the best performance in earlier work in the subject[2,5,7]. For comparison, an SVM classifier from scikit-learn will also be used. \n",
    "\n",
    "For a customer facing device or service, a high level of accuracy is required. As such, an accuracy of at least 0.95 will be defined as a requirement to answer this question. A comparable level of precision and recall is desired, though no specific requirement will be stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifiers with HR features\n",
    "\n",
    "First, a decision tree classifier is used, trained using only HR features extracted from the raw data. These features are: HR mean, HR standard deviation, HR normalised mean and HR normalised standard deviation. Using only these features, the classifier reaches an accuracy level of 0.8874. While it's a relatively good performance, it's below the stated requirement. As can be seen from the confusion matrix, there is significant spill over of prediction into neighbouring  categories, eg. light categories being classified as moderate, and although low, there is also mislabelling into the furthest category, eg. from light to vigorous. This means that although HR might be a reasonably good predictor of activity intensity, it's not accurate enough for the purpose of this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tclf = tree.DecisionTreeClassifier()\n",
    "\n",
    "features_used = ['hr_mean','hr_mean_normal','hr_std','hr_std_normal']\n",
    "label_col = 'act_level'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, ['light','moderate','vigorous'], \"Confusion Matrix for Decision Tree using HR features only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same set of features is used to train an SVM classifier. Performance metrics are on a similar level, with a slightly lower accuracy of 0.8739. Similar behaviour as before can be observed in the confusion matrix. This serves to confirm the findings of the previous test, and the need to employ additional features for activity level identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclf = svm.SVC()\n",
    "\n",
    "features_used = ['hr_mean','hr_mean_normal','hr_std','hr_std_normal']\n",
    "label_col = 'act_level'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "vclf.fit(train_data, train_labels)\n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = vclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, ['light','moderate','vigorous'], \"Confusion Matrix for SVM using HR features only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifiers with HR and Accelerometer features\n",
    "\n",
    "Since the previous two tests demonstrated that HR data is not enough to accurately classify activities by their intensity levels, the training data will now be supplemented with features extracted from accelerometer readings. The features that will be added are: mean and standard deviation of readings for each axis of each accelerometer location. The same classifiers will be trained using this supplemented training set and performance will be measured and compared to the previous results.\n",
    "\n",
    "First, the procedure is carried out using the decision tree classifier. Improvements are apparent by looking at the confusion matrix, as mislabelling now only ocurrs into neighbouring categories. The overall amount of mislabelled data is lower as well. The accuracy level is now 0.9859, which meets the stated requirements. Precision, recall and F-score values are very close.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['hr_mean','hr_mean_normal','hr_std','hr_std_normal',\n",
    "                    'hand_acc_x_mean','hand_acc_x_std','hand_acc_y_mean','hand_acc_y_std','hand_acc_z_mean','hand_acc_z_std',\n",
    "                    'chest_acc_x_mean','chest_acc_x_std','chest_acc_y_mean','chest_acc_y_std','chest_acc_z_mean','chest_acc_z_std',\n",
    "                    'ankle_acc_x_mean','ankle_acc_x_std','ankle_acc_y_mean','ankle_acc_y_std','ankle_acc_z_mean','ankle_acc_z_std'\n",
    "                ]\n",
    "label_col = 'act_level'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, ['light','moderate','vigorous'], \"Confusion Matrix for Decision Tree using HR  and Accelerometer features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the SVM classifier is also trained using the supplemented training set. It shows even better performance than the decision tree classifier, with accuracy = 0.9994, as well as precision, recall and F-score. This shows that supplementing HR data with accelerometer readings might be a viable solution to be able to classify activity intensity level with an almost perfect accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['hr_mean','hr_mean_normal','hr_std','hr_std_normal',\n",
    "                    'hand_acc_x_mean','hand_acc_x_std','hand_acc_y_mean','hand_acc_y_std','hand_acc_z_mean','hand_acc_z_std',\n",
    "                    'chest_acc_x_mean','chest_acc_x_std','chest_acc_y_mean','chest_acc_y_std','chest_acc_z_mean','chest_acc_z_std',\n",
    "                    'ankle_acc_x_mean','ankle_acc_x_std','ankle_acc_y_mean','ankle_acc_y_std','ankle_acc_z_mean','ankle_acc_z_std'\n",
    "                ]\n",
    "label_col = 'act_level'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "vclf.fit(train_data, train_labels)\n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = vclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, ['light','moderate','vigorous'], \"Confusion Matrix for SVM using HR and Accelerometer features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of these tests demonstrate that while HR readings are enough to perform reasonably good classification of activity intensity levels, in order to reach an accuracy level that is better suited to a commercial application, it's necessary to supplement this data with additional sensor information, for which accelerometer readings show great performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second objective for this report is to compare the performance of different sensing devices present in the dataset, when applied to the identification of physical activities. Specifically, the contribution of features extracted from accelerometer and gyrosocope readings when training a classifier will be measured and compared. \n",
    "\n",
    "\n",
    "#### Training Classifiers using  only Accelerometer features\n",
    "First, a decision tree classifier will be trained using features extracted from accelerometer readings. These features are: mean, standard deviation and correlation of axis pairings of each axis for every sensor location, for a total of 27 features. Using this training set, the decision tree classifier is able to reach an accuracy of 0.9795, with comparable levels of precision and recall. This demonstrates using only accelerometer data allows for very good classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['hand_acc_x_mean','hand_acc_x_std','hand_acc_y_mean','hand_acc_y_std','hand_acc_z_mean','hand_acc_z_std',\n",
    "                    'chest_acc_x_mean','chest_acc_x_std','chest_acc_y_mean','chest_acc_y_std','chest_acc_z_mean','chest_acc_z_std',\n",
    "                    'ankle_acc_x_mean','ankle_acc_x_std','ankle_acc_y_mean','ankle_acc_y_std','ankle_acc_z_mean','ankle_acc_z_std',\n",
    "                    'hand_acc_xy_cor','hand_acc_yz_cor','hand_acc_xz_cor','chest_acc_xy_cor','chest_acc_yz_cor','chest_acc_xz_cor',\n",
    "                    'ankle_acc_xy_cor','ankle_acc_yz_cor','ankle_acc_xz_cor'\n",
    "                ]\n",
    "label_col = 'activity_id'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, False, \"Confusion Matrix for Decision Tree using Accelerometer features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifiers using  only Gyroscope features\n",
    "\n",
    "Next, the decision tree classifier is trained using features extracted from gyroscope readings. The feature set is very similar to the previous one, but applied to the gyroscope data found in the dataset: mean, standard deviation and correlation of axis pairings of each axis for every sensor location, for a total of 27 featues. \n",
    "\n",
    "With this training set, the decision tree classifier is only able to reach an accuracy of 0.8973, which while a resonably good level of performance, is considerably lower than that of the accelerometer, and not ideal for the application considered in this report, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['hand_gyr_x_mean','hand_gyr_x_std','hand_gyr_y_mean','hand_gyr_y_std','hand_gyr_z_mean','hand_gyr_z_std',\n",
    "                    'chest_gyr_x_mean','chest_gyr_x_std','chest_gyr_y_mean','chest_gyr_y_std','chest_gyr_z_mean','chest_gyr_z_std',\n",
    "                    'ankle_gyr_x_mean','ankle_gyr_x_std','ankle_gyr_y_mean','ankle_gyr_y_std','ankle_gyr_z_mean','ankle_gyr_z_std',\n",
    "                    'hand_gyr_xy_cor','hand_gyr_yz_cor','hand_gyr_xz_cor','chest_gyr_xy_cor','chest_gyr_yz_cor','chest_gyr_xz_cor',\n",
    "                    'ankle_gyr_xy_cor','ankle_gyr_yz_cor','ankle_gyr_xz_cor']\n",
    "label_col = 'activity_id'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, False, \"Confusion Matrix for Decision Tree using Gyroscope features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifiers using both Accelerometer and Gyroscope features\n",
    "\n",
    "Finally, a test is performed employing a training set composed of both accelerometer and gyroscope features, to measure the performance impact of using both sets of sensors. This means a total of 54 features is used to train the decision tree classifier.\n",
    "\n",
    "Surprisingly, using this new training set, the classifier only reaches an accuracy of 0.9819, a marginal improvement over the 0.9795 reached when it was trained using only accelerometer data. The addition of the gyroscope data had an effectively negligible effect on the classifier's performance. From this, it can be concluded that, at least with the current selection of features, the addition of a gyroscope sensor to an accelerometer does not produce enough gains to justify its use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['hand_acc_x_mean','hand_acc_x_std','hand_acc_y_mean','hand_acc_y_std','hand_acc_z_mean','hand_acc_z_std',\n",
    "                    'chest_acc_x_mean','chest_acc_x_std','chest_acc_y_mean','chest_acc_y_std','chest_acc_z_mean','chest_acc_z_std',\n",
    "                    'ankle_acc_x_mean','ankle_acc_x_std','ankle_acc_y_mean','ankle_acc_y_std','ankle_acc_z_mean','ankle_acc_z_std',\n",
    "                    'hand_acc_xy_cor','hand_acc_yz_cor','hand_acc_xz_cor','chest_acc_xy_cor','chest_acc_yz_cor','chest_acc_xz_cor',\n",
    "                    'ankle_acc_xy_cor','ankle_acc_yz_cor','ankle_acc_xz_cor',\n",
    "                 'hand_gyr_x_mean','hand_gyr_x_std','hand_gyr_y_mean','hand_gyr_y_std','hand_gyr_z_mean','hand_gyr_z_std',\n",
    "                    'chest_gyr_x_mean','chest_gyr_x_std','chest_gyr_y_mean','chest_gyr_y_std','chest_gyr_z_mean','chest_gyr_z_std',\n",
    "                    'ankle_gyr_x_mean','ankle_gyr_x_std','ankle_gyr_y_mean','ankle_gyr_y_std','ankle_gyr_z_mean','ankle_gyr_z_std',\n",
    "                    'hand_gyr_xy_cor','hand_gyr_yz_cor','hand_gyr_xz_cor','chest_gyr_xy_cor','chest_gyr_yz_cor','chest_gyr_xz_cor',\n",
    "                    'ankle_gyr_xy_cor','ankle_gyr_yz_cor','ankle_gyr_xz_cor'\n",
    "                ]\n",
    "label_col = 'activity_id'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, False, \"Confusion Matrix for Decision Tree using Accelerometer and Gyroscope features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous set of tests, it can be concluded that the most efficient sensor setup for a fitness tracking device, might be simply the use of an accelerometer, as it provides a very good level of prediction accuracy, while the addition of gyroscope simply does not provide enough of an improvement to justify the increased complexity of the device, as its effect on performance is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective 3\n",
    "\n",
    "The third objective of this report is to compare the performance of the different sensor locations provided in the PAMAP2 dataset: hand, ankle and chest sensors. Having this information will help the design of better tracking devices, as it will allow designers to select the ideal type of device to create. \n",
    "\n",
    "In the following tests, sensor data from each of these locations will be used individually to train a decision tree classifier, and the performance for each will be measured and compared. Since in the last section of this report it was found that accelerometer data provides the best prediction performance for classifiers, only those features will be used in these tests.\n",
    "\n",
    "#### Training Classifiers using Hand Accelerometer features\n",
    "\n",
    "For the first test, features extracted from accelerometer readings from a sensor located in the subject's hand are used to train a decision tree classifier. These features are mean, standard deviation and correlation of each pairing of the axes, a total of 9 features. Using this training set, the classifier reaches an accuracy of 0.9215, which is very good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['hand_acc_x_mean','hand_acc_x_std','hand_acc_y_mean','hand_acc_y_std','hand_acc_z_mean','hand_acc_z_std',\n",
    "                    'hand_acc_xy_cor','hand_acc_yz_cor','hand_acc_xz_cor']\n",
    "label_col = 'activity_id'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, False, \"Confusion Matrix for Decision Tree using Hand Accelerometer features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifiers using Chest Accelerometer features\n",
    "\n",
    "In the next test, the same feature set is used to train the classifier, except the data are extracted from the chest sensor. With this training set, the classifier reaches an accuracy of 0.9288, slightly better than that achieved with the hand sensor data. Interestingly, recall and F-score have a more marked improvement in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['chest_acc_x_mean','chest_acc_x_std','chest_acc_y_mean','chest_acc_y_std','chest_acc_z_mean','chest_acc_z_std',\n",
    "                    'chest_acc_xy_cor','chest_acc_yz_cor','chest_acc_xz_cor']\n",
    "label_col = 'activity_id'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, False, \"Confusion Matrix for Decision Tree using Chest Accelerometer features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifiers using Ankle Accelerometer features\n",
    "\n",
    "Finally, the test is repeated using the feature set extracted from the ankle accelerometer data. This time, the classifier reaches an accuracy of 0.9036, lower than the other two locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = ['ankle_acc_x_mean','ankle_acc_x_std','ankle_acc_y_mean','ankle_acc_y_std','ankle_acc_z_mean','ankle_acc_z_std',\n",
    "                    'ankle_acc_xy_cor','ankle_acc_yz_cor','ankle_acc_xz_cor']\n",
    "label_col = 'activity_id'\n",
    "\n",
    "train_data = np.array(dev_data_df.loc[:, features_used])\n",
    "train_labels = np.array(dev_data_df.loc[:, label_col])\n",
    "tclf.fit(train_data, train_labels) \n",
    "\n",
    "test_data = np.array(test_data_df.loc[:, features_used])\n",
    "real_labels = np.array(test_data_df.loc[:, label_col])\n",
    "\n",
    "predict_labels = tclf.predict(test_data)\n",
    "confusion_matrix(predict_labels, real_labels, False, \"Confusion Matrix for Decision Tree using Ankle Accelerometer features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this set of tests, it was found that the location for an accelerometer sensor that provides the best performance for activity prediction is the chest, closely followed by hand, with ankle a more distant third. It can be concluded that a device designed to work while located in the chest of the user would provide the best performance, however a hand device, such as a wristband could still prove practical without much accuracy loss. In this case, the practicality and ease of use of the device may inform the decision further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report's introduction, three objectives were set out that when answered would provide useful, actionable insights that could be applied in the development of activity tracking hardware and/or software. The PAMAP2 dataset was used to explore different posibilities to answer these questions, and the following conclusions were reached:\n",
    "\n",
    "- **Objective 1:** With the goal of activity intensity identification, while Heart Rate sensor reading provide enough data for a reasonably good classification, it is not enough to reach the accuracy of 0.95 that was set out as a requirement for the tests. In order to reach this level of accuracy, it's necessary to supplement the HR data with information from additional sensors. In this case, accelerometer readings were used to successfully reach an accuracy of 0.9994 using an SVM classifier. Including both HR and accelerometer sensors in an activity tracking device would be an effective solution if the goal is the correct classification of activities accordingto their intensity. \n",
    "\n",
    "- **Objective 2:** Sensor readings from accelerometer and gyroscope devices were used separately to extract features to train a classifier model, and the performance reached with each training set was compared. In this test it was found that accelerometer data provided the best results, with an accuracy of 0.9795, as compared to 0.8973 reached with the gyroscope data. Moreover, when employing both accelerometer and gyroscope originated features to train the classifier, the performance boost was negligible. From this it can be concluded that including an accelerometer sensor might be enough when designing an activity tracking device to get a good level of performance. \n",
    "\n",
    "- **Objective 3:** Finally the data from all three sensor locations included in the PAMAP2 dataset was tested, in order to identify the optimal sensor location which provided the best prediction performance. Accelerometer data from each location was used to derive features, and separately trai a classifier. It was found that the chest sensor provided the best accuracy, with 0.9288. However the hand sensor had a just slightly lower accuracy of 0.9215. It should be noted that while solving Objective 2, the accelerometer data for all three sensors was used to train the classifier, and an accuracy of 0.9795 was reached. However wearing all three sensors might not prove to be a practical solution for daily activities. Hence the design of an optimal device must also consider other matters in such as practicality and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. References \n",
    "\n",
    "[1] B. E. Ainsworth, W. L. Haskell, M. C. Whitt, M. L. Irwin, a. M. Swartz, S. J. Strath, W. L. O'Brien, D. R. Bassett, K. H. Schmitz, P. O. Emplaincourt, D. R. Jacobs, and a. S. Leon. Compendium of physical activities: an update of activity codes and MET intensities. Medicine and science in sports and exercise, 32(9 Suppl), pp. 498-504, Sept. 2000.\n",
    "\n",
    "[2] A. Reiss and D. Stricker. Creating and Benchmarking a New Dataset for Physical Activity Monitoring. The 5th Workshop on Affect and Behaviour Related Assistance (ABRA), 2012.\n",
    "\n",
    "[3] PerformedActivitiesSummary.pdf, PAMAP2 Dataset Documentation.\n",
    "\n",
    "[4] SubjectInformation.pdf, PAMAP2 Dataset Documentation.\n",
    "\n",
    "[5] A. Reiss and D. Stricker. Towards Global Aerobic Activity Monitoring. In 4th International Conference on Pervasive Technologies Related to Assistive Environments (PETRA), 2011.\n",
    "\n",
    "[6] N. Ravi, N. Dandekar, P. Mysore, and M. Littman. Activity recognition from accelerometer data. In 17th Conference on Innovative Applications of Artificial Intelligence (IAAI), pp. 1541-1546, 2005.\n",
    "\n",
    "[7] L. Bao and S. Intille. Activity recognition from user-annotated acceleration data. In Proc. 2nd Int. Conf. Pervasive Computing pp. 1-17, 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
